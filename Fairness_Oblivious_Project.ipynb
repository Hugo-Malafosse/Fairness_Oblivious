{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Expenditure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific data used is the [2015 Full Year Consolidated Data File](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181) as well as the [2016 Full Year Consolidated Data File](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-192)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2015 file contains data from rounds 3,4,5 of panel 19 (2014) and rounds 1,2,3 of panel 20 (2015). The 2016 file contains data from rounds 3,4,5 of panel 20 (2015) and rounds 1,2,3 of panel 21 (2016).\n",
    "\n",
    "For this demonstration, three datasets were constructed: one from panel 19, round 5 (used for learning models), one from panel 20, round 3 (used for deployment/testing of model - steps); the other from panel 21, round 3 (used for re-training and deployment/testing of updated model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset, the sensitive attribute is 'RACE' constructed as follows: 'Whites' (privileged class) defined by the features RACEV2X = 1 (White) and HISPANX = 2 (non Hispanic); 'Non-Whites' that included everyone else.  \n",
    "\n",
    "Along with race as the sensitive feature, other features used for modeling include demographics  (such as age, gender, active duty status), physical/mental health assessments, diagnosis codes (such as history of diagnosis of cancer, or diabetes), and limitations (such as cognitive or hearing or vision limitation).\n",
    "\n",
    "The model classification task is to predict whether a person would have 'high' utilization (defined as UTILIZATION >= 10, roughly the average utilization for the considered population). High utilization respondents constituted around 17% of each dataset.\n",
    "\n",
    "To simulate the scenario, each dataset is split into 3 parts: a train, a validation, and a test/deployment part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3.](#Table-of-Contents) Training models on original 2015 Panel 19 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 12:21:47.322976: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Datasets\n",
    "from aif360.datasets import MEPSDataset19\n",
    "from aif360.datasets import MEPSDataset20\n",
    "from aif360.datasets import MEPSDataset21\n",
    "\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Explainers\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Bias mitigation techniques\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "\n",
    "# LIME\n",
    "from aif360.datasets.lime_encoder import LimeEncoder\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load data & create splits for learning/validating/testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "\n",
    "# Graphs libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "from plotly import tools\n",
    "\n",
    "# Libraries to study\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import LFR, Reweighing\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing, RejectOptionClassification\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "\n",
    "# Design libraries\n",
    "from IPython.display import Markdown, display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataset and split into train (50%), validate (30%), and test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset_orig_panel19_train,\n",
    " dataset_orig_panel19_val,\n",
    " dataset_orig_panel19_test) = MEPSDataset19().split([0.5, 0.8], shuffle=True)\n",
    "\n",
    "sens_ind = 0\n",
    "sens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\n",
    "\n",
    "unprivileged_groups = [{sens_attr: v} for v in\n",
    "                       dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\n",
    "privileged_groups = [{sens_attr: v} for v in\n",
    "                     dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               instance weights features                                    \\\n",
       "                                         protected attribute                 \n",
       "                                     AGE                RACE  PCS42  MCS42   \n",
       "instance names                                                               \n",
       "6734                8770.425719     68.0                 0.0  37.82  59.53   \n",
       "12972               2909.245554     70.0                 0.0  43.05  59.07   \n",
       "642                 2142.962735     46.0                 0.0  53.73  48.68   \n",
       "4383               13704.021921     16.0                 1.0  -1.00  -1.00   \n",
       "6011                7310.287974     85.0                 1.0  32.62  43.69   \n",
       "...                         ...      ...                 ...    ...    ...   \n",
       "952                 9696.134287      9.0                 1.0  -1.00  -1.00   \n",
       "5442               21257.994423     46.0                 1.0  55.93  52.08   \n",
       "12733               5211.737528      0.0                 0.0  -1.00  -1.00   \n",
       "251                 1864.222283     14.0                 0.0  -1.00  -1.00   \n",
       "13958                  0.000000     62.0                 1.0  -1.00  -1.00   \n",
       "\n",
       "                                                            ...          \\\n",
       "                                                            ...           \n",
       "               K6SUM42 REGION=1 REGION=2 REGION=3 REGION=4  ... EMPST=4   \n",
       "instance names                                              ...           \n",
       "6734               1.0      0.0      0.0      0.0      1.0  ...     1.0   \n",
       "12972              0.0      0.0      0.0      1.0      0.0  ...     1.0   \n",
       "642               11.0      0.0      0.0      0.0      1.0  ...     0.0   \n",
       "4383              -1.0      0.0      1.0      0.0      0.0  ...     0.0   \n",
       "6011               8.0      0.0      1.0      0.0      0.0  ...     1.0   \n",
       "...                ...      ...      ...      ...      ...  ...     ...   \n",
       "952               -1.0      0.0      0.0      1.0      0.0  ...     0.0   \n",
       "5442               3.0      0.0      0.0      1.0      0.0  ...     1.0   \n",
       "12733             -1.0      0.0      0.0      1.0      0.0  ...     0.0   \n",
       "251               -1.0      0.0      0.0      1.0      0.0  ...     0.0   \n",
       "13958             -1.0      0.0      0.0      1.0      0.0  ...     1.0   \n",
       "\n",
       "                                                                               \\\n",
       "                                                                                \n",
       "               POVCAT=1 POVCAT=2 POVCAT=3 POVCAT=4 POVCAT=5 INSCOV=1 INSCOV=2   \n",
       "instance names                                                                  \n",
       "6734                0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n",
       "12972               0.0      0.0      0.0      1.0      0.0      1.0      0.0   \n",
       "642                 1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "4383                0.0      0.0      0.0      1.0      0.0      1.0      0.0   \n",
       "6011                0.0      0.0      0.0      1.0      0.0      1.0      0.0   \n",
       "...                 ...      ...      ...      ...      ...      ...      ...   \n",
       "952                 0.0      0.0      0.0      0.0      1.0      0.0      1.0   \n",
       "5442                0.0      0.0      0.0      0.0      1.0      1.0      0.0   \n",
       "12733               1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "251                 1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "13958               1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "\n",
       "                        labels  \n",
       "                                \n",
       "               INSCOV=3         \n",
       "instance names                  \n",
       "6734                0.0    1.0  \n",
       "12972               0.0    1.0  \n",
       "642                 0.0    0.0  \n",
       "4383                0.0    0.0  \n",
       "6011                0.0    0.0  \n",
       "...                 ...    ...  \n",
       "952                 0.0    0.0  \n",
       "5442                0.0    1.0  \n",
       "12733               0.0    0.0  \n",
       "251                 0.0    0.0  \n",
       "13958               0.0    0.0  \n",
       "\n",
       "[3166 rows x 140 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orig_panel19_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used throughout the notebook to print out some labels, names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def describe(train=None, val=None, test=None):\n",
    "    if train is not None:\n",
    "        display(Markdown(\"#### Training Dataset shape\"))\n",
    "        print(train.features.shape)\n",
    "    if val is not None:\n",
    "        display(Markdown(\"#### Validation Dataset shape\"))\n",
    "        print(val.features.shape)\n",
    "    display(Markdown(\"#### Test Dataset shape\"))\n",
    "    print(test.features.shape)\n",
    "    display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "    print(test.favorable_label, test.unfavorable_label)\n",
    "    display(Markdown(\"#### Protected attribute names\"))\n",
    "    print(test.protected_attribute_names)\n",
    "    display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "    print(test.privileged_protected_attributes, \n",
    "          test.unprivileged_protected_attributes)\n",
    "    display(Markdown(\"#### Dataset feature names\"))\n",
    "    print(test.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show 2015 dataset details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7915, 138)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Validation Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4749, 138)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Test Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3166, 138)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RACE']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AGE', 'RACE', 'PCS42', 'MCS42', 'K6SUM42', 'REGION=1', 'REGION=2', 'REGION=3', 'REGION=4', 'SEX=1', 'SEX=2', 'MARRY=1', 'MARRY=2', 'MARRY=3', 'MARRY=4', 'MARRY=5', 'MARRY=6', 'MARRY=7', 'MARRY=8', 'MARRY=9', 'MARRY=10', 'FTSTU=-1', 'FTSTU=1', 'FTSTU=2', 'FTSTU=3', 'ACTDTY=1', 'ACTDTY=2', 'ACTDTY=3', 'ACTDTY=4', 'HONRDC=1', 'HONRDC=2', 'HONRDC=3', 'HONRDC=4', 'RTHLTH=-1', 'RTHLTH=1', 'RTHLTH=2', 'RTHLTH=3', 'RTHLTH=4', 'RTHLTH=5', 'MNHLTH=-1', 'MNHLTH=1', 'MNHLTH=2', 'MNHLTH=3', 'MNHLTH=4', 'MNHLTH=5', 'HIBPDX=-1', 'HIBPDX=1', 'HIBPDX=2', 'CHDDX=-1', 'CHDDX=1', 'CHDDX=2', 'ANGIDX=-1', 'ANGIDX=1', 'ANGIDX=2', 'MIDX=-1', 'MIDX=1', 'MIDX=2', 'OHRTDX=-1', 'OHRTDX=1', 'OHRTDX=2', 'STRKDX=-1', 'STRKDX=1', 'STRKDX=2', 'EMPHDX=-1', 'EMPHDX=1', 'EMPHDX=2', 'CHBRON=-1', 'CHBRON=1', 'CHBRON=2', 'CHOLDX=-1', 'CHOLDX=1', 'CHOLDX=2', 'CANCERDX=-1', 'CANCERDX=1', 'CANCERDX=2', 'DIABDX=-1', 'DIABDX=1', 'DIABDX=2', 'JTPAIN=-1', 'JTPAIN=1', 'JTPAIN=2', 'ARTHDX=-1', 'ARTHDX=1', 'ARTHDX=2', 'ARTHTYPE=-1', 'ARTHTYPE=1', 'ARTHTYPE=2', 'ARTHTYPE=3', 'ASTHDX=1', 'ASTHDX=2', 'ADHDADDX=-1', 'ADHDADDX=1', 'ADHDADDX=2', 'PREGNT=-1', 'PREGNT=1', 'PREGNT=2', 'WLKLIM=-1', 'WLKLIM=1', 'WLKLIM=2', 'ACTLIM=-1', 'ACTLIM=1', 'ACTLIM=2', 'SOCLIM=-1', 'SOCLIM=1', 'SOCLIM=2', 'COGLIM=-1', 'COGLIM=1', 'COGLIM=2', 'DFHEAR42=-1', 'DFHEAR42=1', 'DFHEAR42=2', 'DFSEE42=-1', 'DFSEE42=1', 'DFSEE42=2', 'ADSMOK42=-1', 'ADSMOK42=1', 'ADSMOK42=2', 'PHQ242=-1', 'PHQ242=0', 'PHQ242=1', 'PHQ242=2', 'PHQ242=3', 'PHQ242=4', 'PHQ242=5', 'PHQ242=6', 'EMPST=-1', 'EMPST=1', 'EMPST=2', 'EMPST=3', 'EMPST=4', 'POVCAT=1', 'POVCAT=2', 'POVCAT=3', 'POVCAT=4', 'POVCAT=5', 'INSCOV=1', 'INSCOV=2', 'INSCOV=3']\n"
     ]
    }
   ],
   "source": [
    "describe(dataset_orig_panel19_train, dataset_orig_panel19_val, dataset_orig_panel19_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.482305229962759\n"
     ]
    }
   ],
   "source": [
    "metric_orig_panel19_train = BinaryLabelDatasetMetric(\n",
    "        dataset_orig_panel19_train,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "explainer_orig_panel19_train = MetricTextExplainer(metric_orig_panel19_train)\n",
    "\n",
    "print(explainer_orig_panel19_train.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the models have some good metrics (not excellent), so I will use these models for origin models.\n",
    "\n",
    "## <a id='4'>4. Bias and Fairness</a>\n",
    "\n",
    "Today, a problem of the model that can be produce by Machine Learning is bias that data can have. So a question is how to measure those bias and how to avoid them. In python there is a package produced by IBM called [aif360](https://github.com/IBM/AIF360) that can gives us some metrics and algorithms to know if our data / model are bias and to get a fair model.\n",
    "\n",
    "### <a id='4.1'>4.1 Metrics</a>\n",
    "\n",
    "So with aif360 we have some metrics that indicate if our data or model are bias. I will use 5 metrics : \n",
    "* Statistical Parity Difference\n",
    "* Equal Opportunity Difference\n",
    "* Average Absolute Odds Difference\n",
    "* Disparate Impact\n",
    "* Theil Index\n",
    "\n",
    "#### <a id='4.1.1'>4.1.1 Statistical Parity Difference</a>\n",
    "\n",
    "This measure is based on the following formula : \n",
    "\n",
    "$$ Pr(Y=1|D=unprivileged) - Pr(Y=1|D=privileged) $$\n",
    "\n",
    "Here the bias or *statistical imparity* is the difference between the probability that a random individual drawn from unprivileged is labeled 1 (so here that he has more than 50K for income) and the probability that a random individual from privileged is labeled 1.\n",
    "\n",
    "So it has to be close to **0** so it will be fair.\n",
    "\n",
    "Also you can find more details about that here : [One definition of algorithmic fairness: statistical parity](https://jeremykun.com/2015/10/19/one-definition-of-algorithmic-fairness-statistical-parity/)\n",
    "\n",
    "\n",
    "#### <a id='4.1.2'>4.1.2 Equal Opportunity Difference</a>\n",
    "\n",
    "This metric is just a difference between the true positive rate of unprivileged group and the true positive rate of privileged group so it follows this formula :\n",
    "\n",
    "$$ TPR_{D=unprivileged} - TPR_{D=privileged} $$ \n",
    "\n",
    "Same as the previous metric we need it to be close to **0**.\n",
    "\n",
    "#### <a id='4.1.3'>4.1.3 Average Absolute Odds Difference</a>\n",
    "\n",
    "This measure is using both false positive rate and true positive rate to calculate the bias. It's calculating the equality of odds with the next formula :\n",
    "\n",
    "$$ \\frac{1}{2}[|FPR_{D=unprivileged} - FPR_{D=privileged} | + | TPR_{D=unprivileged} - TPR_{D=privileged}|]$$\n",
    "\n",
    "It needs to be equal to **0** to be fair.\n",
    "\n",
    "#### <a id='4.1.4'>4.1.4 Disparate Impact</a>\n",
    "\n",
    "For this metric we use the following formula :\n",
    "\n",
    "$$ \\frac{Pr(Y=1|D=unprivileged)}{Pr(Y=1|D=privileged)} $$\n",
    "\n",
    "Like the first metric we use both probabities of a random individual drawn from unprivileged or privileged with a label of 1 but here it's a ratio. \n",
    "\n",
    "It changes the objective, for the disparate impact it's **1** that we need.\n",
    "\n",
    "#### <a id='4.1.5'>4.1.5 Theil Index</a>\n",
    "\n",
    "This measure is also known as the generalized entropy index but with $\\alpha$ equals to 1 (more informations on [the Wikipedia page](https://en.wikipedia.org/wiki/Generalized_entropy_index)). So we can calculate it with this formula :\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=0}^{n} \\frac{b_i}{\\mu} ln \\frac{b_i}{\\mu} $$ \n",
    "\n",
    "Where $b_i = \\hat{y}_i - y_i + 1 $\n",
    "\n",
    "So it needs to be close to **0** to be fair.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Learning a Logistic Regression (LR) classifier on original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Training LR model on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_orig_panel19_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      LogisticRegression(solver='liblinear', random_state=1))\n",
    "fit_params = {'logisticregression__sample_weight': dataset.instance_weights}\n",
    "\n",
    "lr_orig_panel19 = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Validating LR model on original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used throughout the tutorial to find best threshold using a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def test(dataset, model, thresh_arr):\n",
    "    try:\n",
    "        # sklearn classifier\n",
    "        y_val_pred_prob = model.predict_proba(dataset.features)\n",
    "        pos_ind = np.where(model.classes_ == dataset.favorable_label)[0][0]\n",
    "    except AttributeError:\n",
    "        # aif360 inprocessing algorithm\n",
    "        y_val_pred_prob = model.predict(dataset).scores\n",
    "        pos_ind = 0\n",
    "    \n",
    "    metric_arrs = defaultdict(list)\n",
    "    for thresh in thresh_arr:\n",
    "        y_val_pred = (y_val_pred_prob[:, pos_ind] > thresh).astype(np.float64)\n",
    "\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = y_val_pred\n",
    "        metric = ClassificationMetric(\n",
    "                dataset, dataset_pred,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "        metric_arrs['bal_acc'].append((metric.true_positive_rate()\n",
    "                                     + metric.true_negative_rate()) / 2)\n",
    "        metric_arrs['avg_odds_diff'].append(metric.average_odds_difference())\n",
    "        metric_arrs['disp_imp'].append(metric.disparate_impact())\n",
    "        metric_arrs['stat_par_diff'].append(metric.statistical_parity_difference())\n",
    "        metric_arrs['eq_opp_diff'].append(metric.equal_opportunity_difference())\n",
    "        metric_arrs['theil_ind'].append(metric.theil_index())\n",
    "    \n",
    "    return metric_arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics = test(dataset=dataset_orig_panel19_val,\n",
    "                   model=lr_orig_panel19,\n",
    "                   thresh_arr=thresh_arr)\n",
    "lr_orig_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function to print out accuracy and fairness metrics. This will be used throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_metrics(metrics, thresh_arr):\n",
    "    best_ind = np.argmax(metrics['bal_acc'])\n",
    "    print(\"Threshold corresponding to Best balanced accuracy: {:6.4f}\".format(thresh_arr[best_ind]))\n",
    "    print(\"Best balanced accuracy: {:6.4f}\".format(metrics['bal_acc'][best_ind]))\n",
    "#     disp_imp_at_best_ind = np.abs(1 - np.array(metrics['disp_imp']))[best_ind]\n",
    "    disp_imp_at_best_ind = 1 - min(metrics['disp_imp'][best_ind], 1/metrics['disp_imp'][best_ind])\n",
    "    print(\"Corresponding 1-min(DI, 1/DI) value: {:6.4f}\".format(disp_imp_at_best_ind))\n",
    "    print(\"Corresponding average odds difference value: {:6.4f}\".format(metrics['avg_odds_diff'][best_ind]))\n",
    "    print(\"Corresponding statistical parity difference value: {:6.4f}\".format(metrics['stat_par_diff'][best_ind]))\n",
    "    print(\"Corresponding equal opportunity difference value: {:6.4f}\".format(metrics['eq_opp_diff'][best_ind]))\n",
    "    print(\"Corresponding Theil index value: {:6.4f}\".format(metrics['theil_ind'][best_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.1900\n",
      "Best balanced accuracy: 0.7627\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.6066\n",
      "Corresponding average odds difference value: -0.1831\n",
      "Corresponding statistical parity difference value: -0.2643\n",
      "Corresponding equal opportunity difference value: -0.1608\n",
      "Corresponding Theil index value: 0.0936\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. Testing LR model on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_orig_metrics = test(dataset=dataset_orig_panel19_test,\n",
    "                       model=lr_orig_panel19,\n",
    "                       thresh_arr=[thresh_arr[lr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.1900\n",
      "Best balanced accuracy: 0.7759\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.5738\n",
      "Corresponding average odds difference value: -0.2057\n",
      "Corresponding statistical parity difference value: -0.2612\n",
      "Corresponding equal opportunity difference value: -0.2228\n",
      "Corresponding Theil index value: 0.0921\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(lr_orig_metrics, [thresh_arr[lr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the fairness metrics displayed above, the value should be close to '0' for fairness.\n",
    "\n",
    "1-min(DI, 1/DI) < 0.2 is typically desired for classifier predictions to be fair.\n",
    "\n",
    "However, for a logistic regression classifier trained with original training data, at the best classification rate, this is quite high. This implies unfairness.\n",
    "\n",
    "Similarly, $\\text{average odds difference} = \\frac{(FPR_{unpriv}-FPR_{priv})+(TPR_{unpriv}-TPR_{priv})}{2}$ must be close to zero for the classifier to be fair.\n",
    "\n",
    "Again, the results for this classifier-data combination are still high. This still implies unfairness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Learning a Random Forest (RF) classifier on original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. Training RF model on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_orig_panel19_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      RandomForestClassifier(n_estimators=500, min_samples_leaf=25))\n",
    "fit_params = {'randomforestclassifier__sample_weight': dataset.instance_weights}\n",
    "rf_orig_panel19 = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Validating RF model on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics = test(dataset=dataset_orig_panel19_val,\n",
    "                   model=rf_orig_panel19,\n",
    "                   thresh_arr=thresh_arr)\n",
    "rf_orig_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.2300\n",
      "Best balanced accuracy: 0.7727\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.4891\n",
      "Corresponding average odds difference value: -0.1174\n",
      "Corresponding statistical parity difference value: -0.1952\n",
      "Corresponding equal opportunity difference value: -0.1076\n",
      "Corresponding Theil index value: 0.0893\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3. Testing RF model on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_orig_metrics = test(dataset=dataset_orig_panel19_test,\n",
    "                       model=rf_orig_panel19,\n",
    "                       thresh_arr=[thresh_arr[rf_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.2300\n",
      "Best balanced accuracy: 0.7640\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.5040\n",
      "Corresponding average odds difference value: -0.1346\n",
      "Corresponding statistical parity difference value: -0.2121\n",
      "Corresponding equal opportunity difference value: -0.1141\n",
      "Corresponding Theil index value: 0.0944\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(rf_orig_metrics, [thresh_arr[rf_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case of the logistic regression classifier learned on the original data, the fairness metrics for the random forest classifier have values that are quite far from 0.\n",
    "\n",
    "For example, 1 - min(DI, 1/DI) has a value of over 0.5 as opposed to the desired value of < 0.2.\n",
    "\n",
    "This indicates that the random forest classifier learned on the original data is also unfair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.](#Table-of-Contents) Bias mitigation using pre-processing technique - Reweighing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_panel19_train = RW.fit_transform(dataset_orig_panel19_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 1.0\n"
     ]
    }
   ],
   "source": [
    "metric_transf_panel19_train = BinaryLabelDatasetMetric(\n",
    "        dataset_transf_panel19_train,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "explainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\n",
    "\n",
    "print(explainer_transf_panel19_train.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Learning a Logistic Regression (LR) classifier on data transformed by reweighing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Training LR model after reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_transf_panel19_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      LogisticRegression(solver='liblinear', random_state=1))\n",
    "fit_params = {'logisticregression__sample_weight': dataset.instance_weights}\n",
    "lr_transf_panel19 = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Validating  LR model after reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics = test(dataset=dataset_orig_panel19_val,\n",
    "                   model=lr_transf_panel19,\n",
    "                   thresh_arr=thresh_arr)\n",
    "lr_transf_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.2200\n",
      "Best balanced accuracy: 0.7581\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.2939\n",
      "Corresponding average odds difference value: -0.0084\n",
      "Corresponding statistical parity difference value: -0.0992\n",
      "Corresponding equal opportunity difference value: 0.0242\n",
      "Corresponding Theil index value: 0.0938\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Testing  LR model after reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_transf_metrics = test(dataset=dataset_orig_panel19_test,\n",
    "                         model=lr_transf_panel19,\n",
    "                         thresh_arr=[thresh_arr[lr_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.2200\n",
      "Best balanced accuracy: 0.7539\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.2482\n",
      "Corresponding average odds difference value: -0.0151\n",
      "Corresponding statistical parity difference value: -0.0872\n",
      "Corresponding equal opportunity difference value: -0.0035\n",
      "Corresponding Theil index value: 0.0966\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(lr_transf_metrics, [thresh_arr[lr_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fairness metrics for the logistic regression model learned after reweighing are well improved, and thus the model is much more fair relative to the logistic regression model learned from the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Learning a Random Forest (RF) classifier on data transformed by reweighing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1. Training  RF model after reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_transf_panel19_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      RandomForestClassifier(n_estimators=500, min_samples_leaf=25))\n",
    "fit_params = {'randomforestclassifier__sample_weight': dataset.instance_weights}\n",
    "rf_transf_panel19 = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2. Validating  RF model after reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics = test(dataset=dataset_orig_panel19_val,\n",
    "                   model=rf_transf_panel19,\n",
    "                   thresh_arr=thresh_arr)\n",
    "rf_transf_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.2300\n",
      "Best balanced accuracy: 0.7721\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.4142\n",
      "Corresponding average odds difference value: -0.0827\n",
      "Corresponding statistical parity difference value: -0.1623\n",
      "Corresponding equal opportunity difference value: -0.0713\n",
      "Corresponding Theil index value: 0.0882\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3. Testing  RF model after reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_transf_metrics = test(dataset=dataset_orig_panel19_test,\n",
    "                         model=rf_transf_panel19,\n",
    "                         thresh_arr=[thresh_arr[rf_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.2300\n",
      "Best balanced accuracy: 0.7644\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.4336\n",
      "Corresponding average odds difference value: -0.1014\n",
      "Corresponding statistical parity difference value: -0.1776\n",
      "Corresponding equal opportunity difference value: -0.0842\n",
      "Corresponding Theil index value: 0.0938\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(rf_transf_metrics, [thresh_arr[rf_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the model learned from the transformed data is fairer than that learned from the original data. However, the random forest model learned from the transformed data is still relatively unfair as compared to the logistic regression model learned from the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this first model is biased. The next step is to answer the question : How to fix it ?\n",
    "\n",
    "### <a id='4.2'>4.2 How to fix it ?</a>\n",
    "\n",
    "AIF360 use 3 types of algorithms :\n",
    "* Pre-processing algorithms : they are used before training the model\n",
    "* In-processing algorithms : they are fair classifiers so it's during the training\n",
    "* Post-processing algorithms : they are used after training the model\n",
    "\n",
    "![Fairness pipeline](http://image.noelshack.com/fichiers/2018/50/1/1544437769-fairness-pipeline.png)\n",
    "\n",
    "This is the **fairness pipeline**. An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming\n",
    "it into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining\n",
    "predictions from this classifier. Metrics can be calculated on the original, transformed, and predicted datasets as well as between the\n",
    "transformed and predicted datasets. Many other instantiations are also possible (more information on [the aif360 paper](https://arxiv.org/pdf/1810.01943.pdf)).\n",
    "\n",
    "#### <a id='4.2.1'>4.2.1 Pre-processing algorithms </a>\n",
    "\n",
    "There are 4 pre-processing algorithms but for 3 of them **there is a problem** : it will work only if the dataset have one protected attribute but here we have 2 : *Sex* and *Race*. But let's have a quick intro for all of them and at the end I will use the one that work for my case.\n",
    "\n",
    "Because this notebook is an exploration of fairness into models I will only use 1 protected attribute : Victim Race, it will allow me to use almost all the algorithms (but for your information the *Reweighing* algo can works with all protected so that's why I will create a function that gives me the protected and unprotected attributes.\n",
    "\n",
    "You can find all the code information on [the documentation](https://aif360.readthedocs.io/en/latest/modules/preprocessing.html#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(data, selected_attr=None):\n",
    "    unprivileged_groups = []\n",
    "    privileged_groups = []\n",
    "    if selected_attr == None:\n",
    "        selected_attr = data.protected_attribute_names\n",
    "    \n",
    "    for attr in selected_attr:\n",
    "            idx = data.protected_attribute_names.index(attr)\n",
    "            privileged_groups.append({attr:data.privileged_protected_attributes[idx]}) \n",
    "            unprivileged_groups.append({attr:data.unprivileged_protected_attributes[idx]}) \n",
    "\n",
    "    return privileged_groups, unprivileged_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
    "\n",
    "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
    "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_metrics(dataset, pred, pred_is_dataset=False):\n",
    "    if pred_is_dataset:\n",
    "        dataset_pred = pred\n",
    "    else:\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = pred\n",
    "    \n",
    "    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference',  'disparate_impact', 'theil_index']\n",
    "    obj_fairness = [[0,0,0,1,0]]\n",
    "    \n",
    "    fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)\n",
    "    \n",
    "    for attr in dataset_pred.protected_attribute_names:\n",
    "        idx = dataset_pred.protected_attribute_names.index(attr)\n",
    "        privileged_groups =  [{attr:dataset_pred.privileged_protected_attributes[idx][0]}] \n",
    "        unprivileged_groups = [{attr:dataset_pred.unprivileged_protected_attributes[idx][0]}] \n",
    "        \n",
    "        classified_metric = ClassificationMetric(dataset, \n",
    "                                                     dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        metric_pred = BinaryLabelDatasetMetric(dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        acc = classified_metric.accuracy()\n",
    "\n",
    "        row = pd.DataFrame([[metric_pred.mean_difference(),\n",
    "                                classified_metric.equal_opportunity_difference(),\n",
    "                                classified_metric.average_abs_odds_difference(),\n",
    "                                metric_pred.disparate_impact(),\n",
    "                                classified_metric.theil_index()]],\n",
    "                           columns  = cols,\n",
    "                           index = [attr]\n",
    "                          )\n",
    "        fair_metrics = fair_metrics.append(row)    \n",
    "    \n",
    "    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)\n",
    "        \n",
    "    return fair_metrics\n",
    "\n",
    "def plot_fair_metrics(fair_metrics):\n",
    "    fig, ax = plt.subplots(figsize=(20,4), ncols=5, nrows=1)\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "        left    =  0.125, \n",
    "        bottom  =  0.1, \n",
    "        right   =  0.9, \n",
    "        top     =  0.9, \n",
    "        wspace  =  .5, \n",
    "        hspace  =  1.1\n",
    "    )\n",
    "\n",
    "    y_title_margin = 1.2\n",
    "\n",
    "    plt.suptitle(\"Fairness metrics\", y = 1.09, fontsize=20)\n",
    "    sns.set(style=\"dark\")\n",
    "\n",
    "    cols = fair_metrics.columns.values\n",
    "    obj = fair_metrics.loc['objective']\n",
    "    size_rect = [0.2,0.2,0.2,0.4,0.25]\n",
    "    rect = [-0.1,-0.1,-0.1,0.8,0]\n",
    "    bottom = [-1,-1,-1,0,0]\n",
    "    top = [1,1,1,2,1]\n",
    "    bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.2],[0,0.25]]\n",
    "\n",
    "    display(Markdown(\"### Check bias metrics :\"))\n",
    "    display(Markdown(\"A model can be considered bias if just one of these five metrics show that this model is biased.\"))\n",
    "    for attr in fair_metrics.index[1:len(fair_metrics)].values:\n",
    "        display(Markdown(\"#### For the %s attribute :\"%attr))\n",
    "        check = [bound[i][0] < fair_metrics.loc[attr][i] < bound[i][1] for i in range(0,5)]\n",
    "        display(Markdown(\"With default thresholds, bias against unprivileged group detected in **%d** out of 5 metrics\"%(5 - sum(check))))\n",
    "\n",
    "    for i in range(0,5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])\n",
    "        \n",
    "        for j in range(0,len(fair_metrics)-1):\n",
    "            a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]\n",
    "            marg = -0.2 if val < 0 else 0.1\n",
    "            ax.text(a.get_x()+a.get_width()/5, a.get_y()+a.get_height()+marg, round(val, 3), fontsize=15,color='black')\n",
    "\n",
    "        plt.ylim(bottom[i], top[i])\n",
    "        plt.setp(ax.patches, linewidth=0)\n",
    "        ax.add_patch(patches.Rectangle((-5,rect[i]), 10, size_rect[i], alpha=0.3, facecolor=\"green\", linewidth=1, linestyle='solid'))\n",
    "        plt.axhline(obj[i], color='black', alpha=0.3)\n",
    "        plt.title(cols[i])\n",
    "        ax.set_ylabel('')    \n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fair_metrics_and_plot(data, model, plot=True, model_aif=False):\n",
    "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
    "    # fair_metrics function available in the metrics.py file\n",
    "    fair = fair_metrics(data, pred)\n",
    "\n",
    "    if plot:\n",
    "        # plot_fair_metrics function available in the visualisations.py file\n",
    "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
    "        plot_fair_metrics(fair)\n",
    "        display(fair)\n",
    "    \n",
    "    return fair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "##### <a id='4.2.1.1'>4.2.1.1 Disparate impact remover</a>\n",
    "Source : [Feldman et al., 2015](https://dl.acm.org/citation.cfm?doid=2783258.2783311)\n",
    "\n",
    "Disparate impact remover is a preprocessing technique that edits feature values increase group fairness while preserving rank-ordering within groups.\n",
    "If you want to see how it works you can take a look on [an example Notebook from the GitHub of AIF360](https://github.com/IBM/AIF360/blob/master/examples/demo_disparate_impact_remover.ipynb).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig_train, data_orig_test = dataset_orig_panel19_train, dataset_orig_panel19_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score ::  0.6614021923764136\n"
     ]
    }
   ],
   "source": [
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "DIR = DisparateImpactRemover()\n",
    "data_transf_train = DIR.fit_transform(dataset_orig_panel19_train)\n",
    "\n",
    "rf_transf = RandomForestClassifier().fit(data_transf_train.features, \n",
    "                     data_transf_train.labels.ravel(), \n",
    "                     sample_weight=data_transf_train.instance_weights)\n",
    "\n",
    "data_transf_test = DIR.fit_transform(dataset_orig_panel19_test)\n",
    "fair = get_fair_metrics_and_plot(data_transf_test, rf_transf, plot=False)\n",
    "probs = rf_transf.predict_proba(dataset_orig_panel19_test.features)\n",
    "preds = rf_transf.predict(dataset_orig_panel19_test.features)\n",
    "\n",
    "\n",
    "\n",
    "algo_metrics = add_to_df_algo_metrics(algo_metrics, rf_transf, fair, preds, probs, 'DisparateImpact')\n",
    "\n",
    "y_val_pred = rf_transf.predict(dataset_orig_panel19_val.features)\n",
    "dataset_pred = dataset_orig_panel19_val.copy()\n",
    "dataset_pred.labels = y_val_pred\n",
    "\n",
    "metric = ClassificationMetric(\n",
    "                dataset_orig_panel19_val, dataset_pred,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "acc1 = np.mean( rf_transf.predict(dataset_orig_panel19_val.features) == dataset_orig_panel19_val.labels.ravel())\n",
    "acc = (metric.true_positive_rate()+ metric.true_negative_rate()) / 2\n",
    "\n",
    "print('accuracy score :: ', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "##### <a id='4.2.1.2'>4.2.1.2 Learning fair representations</a>\n",
    "Source : [Zemel et al., 2013](http://proceedings.mlr.press/v28/zemel13.html)\n",
    "\n",
    "Learning fair representations is a pre-processing technique that finds a latent representation which encodes the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(data_transf_test.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score ::  0.5\n"
     ]
    }
   ],
   "source": [
    "privileged_groups, unprivileged_groups = get_attributes(dataset_orig_panel19_train, selected_attr=['RACE'])\n",
    "\n",
    "\n",
    "LFR_model = LFR(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups, k=1, verbose=0)\n",
    "# LFR.fit(data_orig_train)\n",
    "data_transf_train = LFR_model.fit_transform(dataset_orig_panel19_train)\n",
    "\n",
    "# Train and save the model\n",
    "rf_transf = RandomForestClassifier().fit(data_transf_train.features, \n",
    "                     data_transf_train.labels.ravel(), \n",
    "                     sample_weight=data_transf_train.instance_weights)\n",
    "\n",
    "data_transf_test = LFR_model.transform(dataset_orig_panel19_test)\n",
    "fair = get_fair_metrics_and_plot(data_transf_test, rf_transf, plot=False)\n",
    "probs = rf_transf.predict_proba(dataset_orig_panel19_test.features)\n",
    "preds = rf_transf.predict(dataset_orig_panel19_test.features)\n",
    "\n",
    "algo_metrics = add_to_df_algo_metrics(algo_metrics, rf_transf, fair, preds, probs, 'LFR')\n",
    "\n",
    "data_transf_test\n",
    "\n",
    "y_val_pred = rf_transf.predict(dataset_orig_panel19_val.features)\n",
    "dataset_pred = dataset_orig_panel19_val.copy()\n",
    "dataset_pred.labels = y_val_pred\n",
    "\n",
    "metric = ClassificationMetric(\n",
    "                dataset_orig_panel19_val, dataset_pred,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "acc1 = np.mean( rf_transf.predict(dataset_orig_panel19_val.features) == dataset_orig_panel19_val.labels.ravel())\n",
    "acc = (metric.true_positive_rate()+ metric.true_negative_rate()) / 2\n",
    "\n",
    "print('accuracy score :: ', acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "##### <a id='4.2.1.3'>4.2.1.3 Optimized preprocessing</a>\n",
    "Source : [Calmon et al., 2017](http://papers.nips.cc/paper/6988-optimized-pre-processing-for-discrimination-prevention)\n",
    "\n",
    "Optimized preprocessing is a preprocessing technique that learns a probabilistic transformation that edits the features and labels in the data with group fairness, individual distortion, and data fidelity constraints and objectives.\n",
    "There is also [a demo notebook on the aif360 GitHub](https://github.com/IBM/AIF360/blob/master/examples/demo_optim_data_preproc.ipynb).\n",
    "\n",
    "*To be honest I tried to work with this one but it's more complicated : it uses options that you have to configure yourself and I don't really find how to choose it. Also it use an Optimizer and I didn't find how to build this class. (I didn't read the paper about this algorithm)*\n",
    "*****\n",
    "##### <a id='4.2.1.4'>4.2.1.4 Reweighing</a>\n",
    "Source : [Kamiran and Calders, 2012](https://link.springer.com/article/10.1007%2Fs10115-011-0463-8)\n",
    "\n",
    "Reweighing is a preprocessing technique that Weights the examples in each (group, label) combination differently to ensure fairness before classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score ::  0.6660902444860455\n"
     ]
    }
   ],
   "source": [
    "privileged_groups, unprivileged_groups = get_attributes(data_orig_train, selected_attr=['RACE'])\n",
    "t0 = time()\n",
    "\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "# RW.fit(data_orig_train)\n",
    "data_transf_train = RW.fit_transform(data_orig_train)\n",
    "\n",
    "# Train and save the model\n",
    "rf_transf = RandomForestClassifier().fit(data_transf_train.features, \n",
    "                     data_transf_train.labels.ravel(), \n",
    "                     sample_weight=data_transf_train.instance_weights)\n",
    "\n",
    "data_transf_test = RW.transform(data_orig_test)\n",
    "fair = get_fair_metrics_and_plot(data_orig_test, rf_transf, plot=False)\n",
    "probs = rf_transf.predict_proba(data_orig_test.features)\n",
    "preds = rf_transf.predict(data_orig_test.features)\n",
    "\n",
    "\n",
    "algo_metrics = add_to_df_algo_metrics(algo_metrics, rf_transf, fair, preds, probs, 'Reweighing')\n",
    "\n",
    "y_val_pred = rf_transf.predict(dataset_orig_panel19_val.features)\n",
    "dataset_pred = dataset_orig_panel19_val.copy()\n",
    "dataset_pred.labels = y_val_pred\n",
    "\n",
    "metric = ClassificationMetric(\n",
    "                dataset_orig_panel19_val, dataset_pred,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "acc1 = np.mean( rf_transf.predict(dataset_orig_panel19_val.features) == dataset_orig_panel19_val.labels.ravel())\n",
    "acc = (metric.true_positive_rate()+ metric.true_negative_rate()) / 2\n",
    "\n",
    "print('accuracy score :: ', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name ::  DisparateImpact\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fairness metrics :: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statistical_parity_difference</th>\n",
       "      <th>equal_opportunity_difference</th>\n",
       "      <th>average_abs_odds_difference</th>\n",
       "      <th>disparate_impact</th>\n",
       "      <th>theil_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>objective</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RACE</th>\n",
       "      <td>-0.096713</td>\n",
       "      <td>-0.128292</td>\n",
       "      <td>0.086505</td>\n",
       "      <td>0.379777</td>\n",
       "      <td>0.135121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           statistical_parity_difference  equal_opportunity_difference  \\\n",
       "objective                       0.000000                      0.000000   \n",
       "RACE                           -0.096713                     -0.128292   \n",
       "\n",
       "           average_abs_odds_difference  disparate_impact  theil_index  \n",
       "objective                     0.000000          1.000000     0.000000  \n",
       "RACE                          0.086505          0.379777     0.135121  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name ::  LFR\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fairness metrics :: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statistical_parity_difference</th>\n",
       "      <th>equal_opportunity_difference</th>\n",
       "      <th>average_abs_odds_difference</th>\n",
       "      <th>disparate_impact</th>\n",
       "      <th>theil_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>objective</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RACE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           statistical_parity_difference  equal_opportunity_difference  \\\n",
       "objective                            0.0                           0.0   \n",
       "RACE                                 0.0                           NaN   \n",
       "\n",
       "           average_abs_odds_difference  disparate_impact  theil_index  \n",
       "objective                          0.0               1.0          0.0  \n",
       "RACE                               NaN               NaN          0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name ::  Reweighing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fairness metrics :: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statistical_parity_difference</th>\n",
       "      <th>equal_opportunity_difference</th>\n",
       "      <th>average_abs_odds_difference</th>\n",
       "      <th>disparate_impact</th>\n",
       "      <th>theil_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>objective</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RACE</th>\n",
       "      <td>-0.094389</td>\n",
       "      <td>-0.10969</td>\n",
       "      <td>0.077885</td>\n",
       "      <td>0.388609</td>\n",
       "      <td>0.132464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           statistical_parity_difference  equal_opportunity_difference  \\\n",
       "objective                       0.000000                       0.00000   \n",
       "RACE                           -0.094389                      -0.10969   \n",
       "\n",
       "           average_abs_odds_difference  disparate_impact  theil_index  \n",
       "objective                     0.000000          1.000000     0.000000  \n",
       "RACE                          0.077885          0.388609     0.132464  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(algo_metrics)):\n",
    "    print(\"model name :: \", algo_metrics.index[i])\n",
    "    display(\"fairness metrics :: \", algo_metrics[\"fair_metrics\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## [5.](#Table-of-Contents) Bias mitigation using in-processing technique - Prejudice Remover (PR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Learning a Prejudice Remover (PR) model on original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1. Training a PR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PrejudiceRemover(sensitive_attr=sens_attr, eta=25.0)\n",
    "pr_orig_scaler = StandardScaler()\n",
    "\n",
    "dataset = dataset_orig_panel19_train.copy()\n",
    "dataset.features = pr_orig_scaler.fit_transform(dataset.features)\n",
    "\n",
    "pr_orig_panel19 = model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2. Validating PR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.50, 50)\n",
    "\n",
    "dataset = dataset_orig_panel19_val.copy()\n",
    "dataset.features = pr_orig_scaler.transform(dataset.features)\n",
    "\n",
    "val_metrics = test(dataset=dataset,\n",
    "                   model=pr_orig_panel19,\n",
    "                   thresh_arr=thresh_arr)\n",
    "pr_orig_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.1200\n",
      "Best balanced accuracy: 0.6836\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.2262\n",
      "Corresponding average odds difference value: 0.0256\n",
      "Corresponding statistical parity difference value: -0.0828\n",
      "Corresponding equal opportunity difference value: 0.1172\n",
      "Corresponding Theil index value: 0.1119\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3. Testing PR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_orig_panel19_test.copy()\n",
    "dataset.features = pr_orig_scaler.transform(dataset.features)\n",
    "\n",
    "pr_orig_metrics = test(dataset=dataset,\n",
    "                       model=pr_orig_panel19,\n",
    "                       thresh_arr=[thresh_arr[pr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.1200\n",
      "Best balanced accuracy: 0.6880\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.1588\n",
      "Corresponding average odds difference value: 0.0523\n",
      "Corresponding statistical parity difference value: -0.0566\n",
      "Corresponding equal opportunity difference value: 0.1479\n",
      "Corresponding Theil index value: 0.1108\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(pr_orig_metrics, [thresh_arr[pr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case of reweighing, prejudice remover results in a fair model. However, it has come at the expense of relatively lower balanced accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6.](#Table-of-Contents) Summary of Model Learning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>avg_odds_diff</th>\n",
       "      <th>disp_imp</th>\n",
       "      <th>stat_par_diff</th>\n",
       "      <th>eq_opp_diff</th>\n",
       "      <th>theil_ind</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bias Mitigator</th>\n",
       "      <th>Classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.775935</td>\n",
       "      <td>-0.205706</td>\n",
       "      <td>0.426176</td>\n",
       "      <td>-0.261207</td>\n",
       "      <td>-0.222779</td>\n",
       "      <td>0.092122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.764042</td>\n",
       "      <td>-0.134638</td>\n",
       "      <td>0.495973</td>\n",
       "      <td>-0.212121</td>\n",
       "      <td>-0.114076</td>\n",
       "      <td>0.094373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reweighing</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.753893</td>\n",
       "      <td>-0.015104</td>\n",
       "      <td>0.751755</td>\n",
       "      <td>-0.087196</td>\n",
       "      <td>-0.003518</td>\n",
       "      <td>0.096575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reweighing</th>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.764359</td>\n",
       "      <td>-0.101374</td>\n",
       "      <td>0.566427</td>\n",
       "      <td>-0.177582</td>\n",
       "      <td>-0.084247</td>\n",
       "      <td>0.093787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prejudice Remover</th>\n",
       "      <th></th>\n",
       "      <td>0.688028</td>\n",
       "      <td>0.052286</td>\n",
       "      <td>0.841229</td>\n",
       "      <td>-0.056631</td>\n",
       "      <td>0.147869</td>\n",
       "      <td>0.110774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        bal_acc  avg_odds_diff  disp_imp  \\\n",
       "Bias Mitigator    Classifier                                               \n",
       "                  Logistic Regression  0.775935      -0.205706  0.426176   \n",
       "                  Random Forest        0.764042      -0.134638  0.495973   \n",
       "Reweighing        Logistic Regression  0.753893      -0.015104  0.751755   \n",
       "Reweighing        Random Forest        0.764359      -0.101374  0.566427   \n",
       "Prejudice Remover                      0.688028       0.052286  0.841229   \n",
       "\n",
       "                                       stat_par_diff  eq_opp_diff  theil_ind  \n",
       "Bias Mitigator    Classifier                                                  \n",
       "                  Logistic Regression      -0.261207    -0.222779   0.092122  \n",
       "                  Random Forest            -0.212121    -0.114076   0.094373  \n",
       "Reweighing        Logistic Regression      -0.087196    -0.003518   0.096575  \n",
       "Reweighing        Random Forest            -0.177582    -0.084247   0.093787  \n",
       "Prejudice Remover                          -0.056631     0.147869   0.110774  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.multi_sparse', False)\n",
    "results = [lr_orig_metrics, rf_orig_metrics, lr_transf_metrics,\n",
    "           rf_transf_metrics, pr_orig_metrics]\n",
    "debias = pd.Series(['']*2 + ['Reweighing']*2\n",
    "                 + ['Prejudice Remover'],\n",
    "                   name='Bias Mitigator')\n",
    "clf = pd.Series(['Logistic Regression', 'Random Forest']*2 + [''],\n",
    "                name='Classifier')\n",
    "pd.concat([pd.DataFrame(metrics) for metrics in results], axis=0).set_index([debias, clf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the models, the logistic regression model gives the best balance in terms of balanced accuracy and fairness. While the model learnt by prejudice remover is slightly fairer, it has much lower accuracy. All other models are quite unfair compared to the logistic model. Hence, we take the logistic regression model learnt from data transformed by re-weighing and 'deploy' it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [7.](#Table-of-Contents) Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "\n",
    "class  ObliviousData:\n",
    "    def build_O_discrete(self,K,S):\n",
    "        n = math.floor(((np.shape(K))[1])/2) # n = half the samples\n",
    "        self.n = n\n",
    "        self.K=K[:2*n,:2*n] #remove one data point if n is odd\n",
    "        \n",
    "        #Bin S\n",
    "        S_binned = S #assuming that S is discrete and starts at 0\n",
    "        self.S_train = S_binned[0:n]\n",
    "        self.S_cond = S_binned[n:2*n]\n",
    "        \n",
    "        self.S_max = int(max(S_binned)+1) \n",
    "   \n",
    "        #precompute\n",
    "\n",
    "        Ephi = np.mean(K[n:,n:])\n",
    "        \n",
    "        mean_iI = np.zeros((n,self.S_max))\n",
    "        mean_i = np.zeros((n,1))\n",
    "        for i in range(n):\n",
    "            mean_i[i] = np.mean(K[i,n:])\n",
    "            for u in range(self.S_max): \n",
    "                I = n + np.where(self.S_cond==u)[0] # all I in cond which correspond to sensitive value u\n",
    "                mean_iI[i,u] = np.mean(K[i,I])\n",
    "\n",
    "        mean_I = np.zeros((self.S_max,1))\n",
    "        mean_IJ = np.zeros((self.S_max,self.S_max))\n",
    "        for u in range(self.S_max):\n",
    "            I = n + np.where(self.S_cond==u)[0] # all I in cond which correspond to sensitive value u\n",
    "            mean_I[u] = np.mean(K[I,n:])\n",
    "            for v in range(self.S_max):\n",
    "                J = n + np.where(self.S_cond==v)[0] # all I in cond which correspond to sensitive value v\n",
    "                mean_IJ[u,v] = np.mean((K[I,:])[:,J]) \n",
    "\n",
    "\n",
    "        #final loop\n",
    "        O = copy(K[:n,:n]) #Kernel matrix for the first n elements\n",
    "        for i in range(n):\n",
    "            for j in range(i,n):\n",
    "                u = int(self.S_train[i])\n",
    "                v = int(self.S_train[j])\n",
    "    \n",
    "                O[i,j] = O[i,j] - mean_iI[i,v]  - mean_iI[j,u]  + mean_IJ[u,v] + mean_i[j] + mean_i[i] - mean_I[u] - mean_I[v]  + Ephi\n",
    "                O[j,i] = O[i,j]\n",
    "                \n",
    "        self.O = O\n",
    "        self.mean_i = mean_i\n",
    "        self.mean_iI = mean_iI\n",
    "        self.mean_I = mean_I\n",
    "        self.mean_IJ = mean_IJ\n",
    "        self.Ephi = Ephi\n",
    "                \n",
    "        return O\n",
    "    \n",
    "    \n",
    "    def build_Ot_discrete(self,Kt,St): #Kt has size testsample x 2n \n",
    "        Ot = copy(Kt[:,:self.n])\n",
    "        m = (np.shape(Kt))[0]\n",
    "    \n",
    "        mean_test_iI = np.zeros((m,self.S_max))\n",
    "        mean_test_i = np.zeros((m,1))\n",
    "        for i in range(m):\n",
    "            mean_test_i[i] = np.mean(Kt[i,self.n:])\n",
    "            for u in range(self.S_max):\n",
    "                I = self.n + np.where(self.S_cond==u)[0] # all I in cond which correspond to sensitive value u\n",
    "                mean_test_iI[i,u] = np.mean(Kt[i,I])\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(self.n): #not symmetric anymore so we need to go through all\n",
    "                u = int(St[i])\n",
    "                v = int(self.S_train[j]) \n",
    "                Ot[i,j] = Ot[i,j] - mean_test_iI[i,v]- self.mean_iI[j,u]  + self.mean_IJ[u,v]  +  self.mean_i[j] + mean_test_i[i]  - self.mean_I[u] - self.mean_I[v]  + self.Ephi # i corresponds to test, j to train; \n",
    "        self.Ot = Ot\n",
    "        return self.Ot \n",
    "    \n",
    "    \n",
    "    def predict_g(self,Kt, St): #for M-Oblivious \n",
    "        m = (np.shape(Kt))[0]#kt has size testsample x 2n \n",
    "        mean_test_iI = np.zeros((self.n,self.S_max))\n",
    "        mean_test_i = np.zeros((self.n,1))\n",
    "        for i in range(self.n):\n",
    "            mean_test_i[i] = np.mean(self.K[i,self.n:])\n",
    "            for u in range(self.S_max):\n",
    "                I = self.n + np.where(self.S_cond==u)[0] # all i in cond which correspond to sensitive value u\n",
    "                mean_test_iI[i,u] = np.mean(self.K[i,I])\n",
    "        \n",
    "        M_XS = np.matlib.repmat(mean_test_i, 1, self.S_max)-mean_test_iI\n",
    "        gOt=np.zeros((m,self.n))\n",
    "        for i in range(m):\n",
    "            gOt[i,:]=M_XS[:,St[i][0][0]]\n",
    "        \n",
    "        gOt= gOt+Kt[:,:self.n]\n",
    "\n",
    "        return gOt \n",
    "    \n",
    "    \n",
    "\n",
    "    def build_K_rbf(self,X1,X2,sigma=1):\n",
    "        # X1 has n rows = number of samples; X2 has m rows = number of samples \n",
    "        n = (np.shape(X1))[0]\n",
    "        m = (np.shape(X2))[0]\n",
    "        K_1 = np.zeros((n,m))\n",
    "    \n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                K_1[i,j] = np.exp((-1)*(np.dot(X1[i]-X2[j],X1[i]-X2[j]))/sigma)\n",
    "                \n",
    "        return K_1\n",
    "\n",
    "\n",
    "    def build_K_lin(self,X1,X2):\n",
    "        # X1 has n rows = number of samples; X2 has m rows = number of samples \n",
    "        n = (np.shape(X1))[0]\n",
    "        m = (np.shape(X2))[0]\n",
    "        K = np.zeros((n,m))\n",
    "    \n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                K[i,j] = np.dot(X1[i],X2[j])\n",
    "        \n",
    "\n",
    "        return K\n",
    "\n",
    "    \n",
    "    def Omatrix(self):\n",
    "        return self.O\n",
    "    \n",
    "    def Otmatrix(self):\n",
    "        return self.Ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import numpy.matlib\n",
    "from sklearn import svm\n",
    "from collections import namedtuple\n",
    "\n",
    "def generate_truncnorm_samples(n_samples,lower,upper,mu,sigma):\n",
    "    X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "    values = X.rvs(n_samples)\n",
    "    return values\n",
    "\n",
    "def generate_toy_data(n_samples):\n",
    "    sigma =0.5\n",
    "    unique_sensitive_feature_values=[0,1]\n",
    "    max_non_sensitive_feature_value=4.0\n",
    "    min_non_sensitive_feature_value=1.0\n",
    "    mu = 0.5*(max_non_sensitive_feature_value+min_non_sensitive_feature_value)\n",
    "    sensitive_features = [unique_sensitive_feature_values[0]] * n_samples + [unique_sensitive_feature_values[1]] * n_samples \n",
    "    sensitive_features = np.array(sensitive_features)\n",
    "    sensitive_features.shape = (len(sensitive_features), 1)\n",
    "    \n",
    "    Lower = generate_truncnorm_samples(n_samples,min_non_sensitive_feature_value,max_non_sensitive_feature_value,mu,sigma)\n",
    "    Upper = generate_truncnorm_samples(n_samples,min_non_sensitive_feature_value,max_non_sensitive_feature_value,mu,sigma)\n",
    "    non_sensitive_features0=[Lower]+[Upper]\n",
    "    non_sensitive_features0 = np.array(np.hstack(non_sensitive_features0))\n",
    "    non_sensitive_features0.shape=(len(non_sensitive_features0),1)\n",
    "    \n",
    "    \n",
    "    non_sensitive_features=[Lower-stats.bernoulli(0.9).rvs(n_samples)*1]+[Upper+stats.bernoulli(0.9).rvs(n_samples)*1]\n",
    "    non_sensitive_features = np.array(np.hstack(non_sensitive_features))\n",
    "    non_sensitive_features.shape=(len(non_sensitive_features),1)\n",
    "    \n",
    "    X = np.hstack([non_sensitive_features, sensitive_features])\n",
    "    \n",
    "    threshold=mu\n",
    "    Y_Bernoulli_params=np.array(non_sensitive_features0/max_non_sensitive_feature_value).flatten()\n",
    "    Y=np.array([stats.bernoulli(Y_Bernoulli_params[i]).rvs(1) for i in range(len(Y_Bernoulli_params))]).flatten()\n",
    "    True_Y=Y*(np.array((non_sensitive_features0>=threshold)*1).flatten())\n",
    "    Y=Y*(np.array((non_sensitive_features+sensitive_features>=threshold)*1).flatten())\n",
    "    sensitive_feature_id=np.shape(X)[1]-1\n",
    "\n",
    "    return X, Y, sensitive_feature_id, True_Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def estimate_beta_dependence(predicted_labels, sensitive_features, labels):\n",
    "    estimated_beta=0\n",
    "    n = np.size(predicted_labels)\n",
    "    unique_label_freqs =[]\n",
    "    for i in range(len(labels)):\n",
    "        unique_label_freqs.append(np.mean(predicted_labels==labels[i]))\n",
    "\n",
    "    unique_S_features=list(set(sensitive_features))\n",
    "    unique_S_freqs =[]\n",
    "    for i in range(len(unique_S_features)):\n",
    "        unique_S_freqs.append(np.mean(sensitive_features==unique_S_features[i]))\n",
    "\n",
    "    pred_feature_pairs = np.vstack((predicted_labels, sensitive_features)).T\n",
    "     \n",
    "    joint_freqs=[]       \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(unique_S_features)):\n",
    "            pattern=(labels[i],unique_S_features[j])\n",
    "            joint_freq=np.size(np.where(np.sum(np.abs(pred_feature_pairs-np.matlib.repmat(pattern, n, 1)),axis=1)==0))/n\n",
    "            joint_freqs.append(joint_freq)\n",
    "            marginal_label_freq=unique_label_freqs[i]\n",
    "            marginal_S_freq=unique_S_freqs[j]\n",
    "            estimated_beta=estimated_beta+np.abs(marginal_label_freq * marginal_S_freq-joint_freq)\n",
    "    return estimated_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               instance weights features            features features  \\\n",
       "                                         protected attribute            \n",
       "                                     AGE                RACE    PCS42   \n",
       "instance names                                                          \n",
       "6274                9584.405538     56.0                 0.0    24.59   \n",
       "4403               13501.448564     19.0                 1.0    -1.00   \n",
       "8660                   0.000000     73.0                 1.0    -1.00   \n",
       "1220               45482.792820     73.0                 1.0    23.64   \n",
       "12020               4639.875613      2.0                 0.0    -1.00   \n",
       "...                         ...      ...                 ...      ...   \n",
       "13556               8527.049685      7.0                 1.0    -1.00   \n",
       "16232               6809.283500     18.0                 0.0    52.40   \n",
       "279                 3750.437997     19.0                 0.0    56.41   \n",
       "4710                6853.044637     16.0                 0.0    -1.00   \n",
       "5825                5441.955183     75.0                 0.0    -1.00   \n",
       "\n",
       "               features features features features features features  ...  \\\n",
       "                                                                      ...   \n",
       "                  MCS42  K6SUM42 REGION=1 REGION=2 REGION=3 REGION=4  ...   \n",
       "instance names                                                        ...   \n",
       "6274              38.23     11.0      1.0      0.0      0.0      0.0  ...   \n",
       "4403              -1.00     -1.0      1.0      0.0      0.0      0.0  ...   \n",
       "8660              -1.00     -1.0      0.0      0.0      0.0      1.0  ...   \n",
       "1220              63.41      3.0      0.0      1.0      0.0      0.0  ...   \n",
       "12020             -1.00     -1.0      0.0      1.0      0.0      0.0  ...   \n",
       "...                 ...      ...      ...      ...      ...      ...  ...   \n",
       "13556             -1.00     -1.0      0.0      1.0      0.0      0.0  ...   \n",
       "16232             62.66      0.0      0.0      0.0      1.0      0.0  ...   \n",
       "279               50.58      0.0      0.0      0.0      0.0      1.0  ...   \n",
       "4710              -1.00     -1.0      0.0      0.0      0.0      1.0  ...   \n",
       "5825              -1.00     -1.0      0.0      0.0      1.0      0.0  ...   \n",
       "\n",
       "               features features features features features features features  \\\n",
       "                                                                                \n",
       "                EMPST=4 POVCAT=1 POVCAT=2 POVCAT=3 POVCAT=4 POVCAT=5 INSCOV=1   \n",
       "instance names                                                                  \n",
       "6274                1.0      1.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4403                0.0      0.0      0.0      0.0      1.0      0.0      0.0   \n",
       "8660                0.0      0.0      0.0      0.0      0.0      1.0      1.0   \n",
       "1220                1.0      0.0      0.0      0.0      0.0      1.0      1.0   \n",
       "12020               0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n",
       "...                 ...      ...      ...      ...      ...      ...      ...   \n",
       "13556               0.0      0.0      0.0      0.0      1.0      0.0      1.0   \n",
       "16232               1.0      0.0      0.0      0.0      1.0      0.0      0.0   \n",
       "279                 0.0      0.0      0.0      1.0      0.0      0.0      0.0   \n",
       "4710                1.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "5825                1.0      1.0      0.0      0.0      0.0      0.0      1.0   \n",
       "\n",
       "               features features labels  \n",
       "                                         \n",
       "               INSCOV=2 INSCOV=3         \n",
       "instance names                           \n",
       "6274                1.0      0.0    1.0  \n",
       "4403                1.0      0.0    0.0  \n",
       "8660                0.0      0.0    0.0  \n",
       "1220                0.0      0.0    1.0  \n",
       "12020               0.0      0.0    0.0  \n",
       "...                 ...      ...    ...  \n",
       "13556               0.0      0.0    0.0  \n",
       "16232               0.0      1.0    0.0  \n",
       "279                 1.0      0.0    0.0  \n",
       "4710                1.0      0.0    0.0  \n",
       "5825                0.0      0.0    1.0  \n",
       "\n",
       "[7915 rows x 140 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_orig_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig_train.features = data_orig_train.features[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = data_orig_train.features[:3957], data_orig_train.labels[:3957].ravel()\n",
    "X_cond,y_cond = data_orig_train.features[3957:], data_orig_train.labels[3957:].ravel()\n",
    "X_test,y_test = dataset_orig_panel19_val.features, dataset_orig_panel19_val.labels.ravel()\n",
    "\n",
    "\n",
    "\n",
    "X = data_orig_train.features\n",
    "y = data_orig_train.labels.ravel()\n",
    "S_train = data_orig_train.protected_attributes[:3957].ravel()\n",
    "S_train_cond = data_orig_train.protected_attributes[3957:].ravel()\n",
    "S = data_orig_train.protected_attributes.ravel()\n",
    "S_test = dataset_orig_panel19_val.protected_attributes.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels=list(set(list(y_test)))\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5061.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        2854.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot data\n",
    "plt.hist(S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBLIVIOUS SVM\n"
     ]
    }
   ],
   "source": [
    "C_list = [2**v for v in [0,1,2,3,4,5,6,7,8,9,10]]    \n",
    "#Oblivious SVM\n",
    "print('OBLIVIOUS SVM')\n",
    "obl = ObliviousData()\n",
    "K = obl.build_K_lin(np.array(X),np.array(X))\n",
    "O = obl.build_O_discrete(K,S)\n",
    "Kt = obl.build_K_lin(np.array(X_test),np.array(X))\n",
    "Ot = obl.build_Ot_discrete(Kt,np.array(S_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:  1\n",
      "Fitting OBLIVIOUS SVM\n",
      "accuracy score ::  0.6380179263752692\n",
      "Standard Missclassification acc:  0.8492314171404506\n",
      "C:  2\n",
      "Fitting OBLIVIOUS SVM\n",
      "accuracy score ::  0.6393193015343264\n",
      "Standard Missclassification acc:  0.8492314171404506\n",
      "C:  4\n",
      "Fitting OBLIVIOUS SVM\n",
      "accuracy score ::  0.6396221091663462\n",
      "Standard Missclassification acc:  0.8490208464939988\n",
      "C:  8\n",
      "Fitting OBLIVIOUS SVM\n",
      "accuracy score ::  0.6398762107953372\n",
      "Standard Missclassification acc:  0.848389134554643\n",
      "C:  16\n",
      "Fitting OBLIVIOUS SVM\n",
      "accuracy score ::  0.6371983972640676\n",
      "Standard Missclassification acc:  0.8481785639081912\n",
      "C:  32\n",
      "Fitting OBLIVIOUS SVM\n",
      "accuracy score ::  0.6491138604705916\n",
      "Standard Missclassification acc:  0.8500736997262581\n",
      "C:  64\n",
      "Fitting OBLIVIOUS SVM\n",
      "accuracy score ::  0.6534590402828119\n",
      "Standard Missclassification acc:  0.8526005474836807\n",
      "C:  128\n",
      "Fitting OBLIVIOUS SVM\n",
      "accuracy score ::  0.6540832661573122\n",
      "Standard Missclassification acc:  0.8513371236049695\n",
      "C:  256\n",
      "Fitting OBLIVIOUS SVM\n",
      "accuracy score ::  0.659977760082958\n",
      "Standard Missclassification acc:  0.8507054116656139\n",
      "C:  512\n",
      "Fitting OBLIVIOUS SVM\n",
      "accuracy score ::  0.6671320509891105\n",
      "Standard Missclassification acc:  0.8513371236049695\n",
      "C:  1024\n",
      "Fitting OBLIVIOUS SVM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stdacc_list,true_acc_list,betas =[],[],[]\n",
    "    \n",
    "for c in C_list:\n",
    "    print('C: ', c)\n",
    "    clf = svm.SVC(kernel='precomputed', C=c)\n",
    "    print('Fitting OBLIVIOUS SVM')\n",
    "    clf.fit(O,y_train)\n",
    "    \n",
    "    pred_oblv = clf.predict(Ot)\n",
    "\n",
    "    y_val_pred = pred_oblv\n",
    "    dataset_pred = dataset_orig_panel19_val.copy()\n",
    "    dataset_pred.labels = y_val_pred\n",
    "\n",
    "    metric = ClassificationMetric(\n",
    "                    dataset_orig_panel19_val, dataset_pred,\n",
    "                    unprivileged_groups=unprivileged_groups,\n",
    "                    privileged_groups=privileged_groups)\n",
    "\n",
    "    acc1 = np.mean( rf_transf.predict(dataset_orig_panel19_val.features) == dataset_orig_panel19_val.labels.ravel())\n",
    "    acc = (metric.true_positive_rate()+ metric.true_negative_rate()) / 2\n",
    "\n",
    "    print('accuracy score :: ', acc)\n",
    "    \n",
    "    standard_missclassification_error=np.mean((pred_oblv==y_test)*1) # error with respect to observed labels \n",
    "    print('Standard Missclassification acc: ', standard_missclassification_error)\n",
    "    # Dependence measure\n",
    "    #beta = estimate_beta_dependence(pred_oblv,X_test['Pclass'],unique_labels)\n",
    "    #print('Beta-Dependence: ',beta)\n",
    "    stdacc_list.append(standard_missclassification_error)\n",
    "    #betas.append(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bigmac/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bigmac/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bigmac/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bigmac/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bigmac/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score ::  0.6768058720598608\n",
      "LogisticRegression: Accuracy: 0.8515, Best Params: {'logisticregression__C': 1}\n",
      "accuracy score ::  0.6701866901256477\n",
      "SVC: Accuracy: 0.8551, Best Params: {'svc__C': 1, 'svc__kernel': 'rbf'}\n",
      "accuracy score ::  0.6532538838948979\n",
      "DecisionTreeClassifier: Accuracy: 0.8318, Best Params: {'decisiontreeclassifier__max_depth': 10}\n",
      "accuracy score ::  0.6613030799668325\n",
      "RandomForestClassifier: Accuracy: 0.8583, Best Params: {'randomforestclassifier__max_depth': 10, 'randomforestclassifier__n_estimators': 100}\n",
      "accuracy score ::  0.6728872898448529\n",
      "GradientBoostingClassifier: Accuracy: 0.8558, Best Params: {'gradientboostingclassifier__learning_rate': 0.1, 'gradientboostingclassifier__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Exemple de données\n",
    "# X_train, y_train, X_val, y_val = ...\n",
    "\n",
    "models_and_parameters = {\n",
    "    'LogisticRegression': (LogisticRegression(),\n",
    "                           {'logisticregression__C': [0.1, 1, 10]}),\n",
    "    'SVC': (SVC(),\n",
    "            {'svc__C': [0.1, 1, 10], 'svc__kernel': ['linear', 'rbf']}),\n",
    "    'DecisionTreeClassifier': (DecisionTreeClassifier(),\n",
    "                               {'decisiontreeclassifier__max_depth': [None, 10, 20, 30]}),\n",
    "    'RandomForestClassifier': (RandomForestClassifier(),\n",
    "                               {'randomforestclassifier__n_estimators': [10, 50, 100], 'randomforestclassifier__max_depth': [None, 10, 20]}),\n",
    "    'GradientBoostingClassifier': (GradientBoostingClassifier(),\n",
    "                                   {'gradientboostingclassifier__n_estimators': [100, 200], 'gradientboostingclassifier__learning_rate': [0.1, 0.01]})\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, (model, params) in models_and_parameters.items():\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), (model_name.lower(), model)])\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=-1)\n",
    "    \n",
    "    # Formation et optimisation avec X_train, y_train\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation sur X_val, y_val\n",
    "\n",
    "\n",
    "    val_predictions = grid_search.predict(X_test)\n",
    "    val_accuracy = accuracy_score(y_test, val_predictions)\n",
    "    \n",
    "    results[model_name] = f'Accuracy: {val_accuracy:.4f}, Best Params: {grid_search.best_params_}'\n",
    "    print(f\"{model_name}: {results[model_name]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Titanic dataset\n",
    "\n",
    " Titanic models typically rely too heavily on sex as a feature, which is unacceptable in our modern and enlightened world. To try and address this problem we'll;\n",
    " - Assess the bias in the Titainc dataset\n",
    " - Apply reweighing in pre-processing using the new [IBM AIF 360 toolbox]\n",
    " - Train benchmark and \"fair\" models, using both linear (logistic regression) and non-parametric (random forest) models\n",
    " - Compare the performance of the models\n",
    " - Try different techniques for comparing the fairness of the benchmark and \"fair\" models, and compare the effectiveness of the reweighing approach between logistic regression and random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to use type hinting\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "# Modelling. Warnings will be used to silence various model warnings for tidier output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "\n",
    "# Data handling/display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "\n",
    "# IBM's fairness tooolbox:\n",
    "from aif360.datasets import BinaryLabelDataset  # To handle the data\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric  # For calculating metrics\n",
    "from aif360.explainers import MetricTextExplainer  # For explaining metrics\n",
    "from aif360.algorithms.preprocessing import Reweighing  # Preprocessing technique\n",
    "\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('/Users/bigmac/Desktop/train.csv').sample(frac=1, random_state=42)\n",
    "test = pd.read_csv('/Users/bigmac/Desktop/test.csv').sample(frac=1, random_state=42)\n",
    "test.loc[:, 'Survived'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_ = pd.read_csv('/Users/bigmac/Desktop/county_data_abridged.csv').sample(frac=1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['countyFIPS',\n",
       " 'STATEFP',\n",
       " 'COUNTYFP',\n",
       " 'CountyName',\n",
       " 'StateName',\n",
       " 'State',\n",
       " 'lat',\n",
       " 'lon',\n",
       " 'POP_LATITUDE',\n",
       " 'POP_LONGITUDE',\n",
       " 'CensusRegionName',\n",
       " 'CensusDivisionName',\n",
       " 'Rural-UrbanContinuumCode2013',\n",
       " 'PopulationEstimate2018',\n",
       " 'PopTotalMale2017',\n",
       " 'PopTotalFemale2017',\n",
       " 'FracMale2017',\n",
       " 'PopulationEstimate65+2017',\n",
       " 'PopulationDensityperSqMile2010',\n",
       " 'CensusPopulation2010',\n",
       " 'MedianAge2010',\n",
       " '#EligibleforMedicare2018',\n",
       " 'MedicareEnrollment,AgedTot2017',\n",
       " '3-YrDiabetes2015-17',\n",
       " 'DiabetesPercentage',\n",
       " 'HeartDiseaseMortality',\n",
       " 'StrokeMortality',\n",
       " 'Smokers_Percentage',\n",
       " 'RespMortalityRate2014',\n",
       " '% Adults with Obesity',\n",
       " '% Fair or Poor Health',\n",
       " '#FTEHospitalTotal2017',\n",
       " \"TotalM.D.'s,TotNon-FedandFed2017\",\n",
       " '#HospParticipatinginNetwork2017',\n",
       " '#Hospitals',\n",
       " '#ICU_beds',\n",
       " 'dem_to_rep_ratio',\n",
       " 'PopMale<52010',\n",
       " 'PopFmle<52010',\n",
       " 'PopMale5-92010',\n",
       " 'PopFmle5-92010',\n",
       " 'PopMale10-142010',\n",
       " 'PopFmle10-142010',\n",
       " 'PopMale15-192010',\n",
       " 'PopFmle15-192010',\n",
       " 'PopMale20-242010',\n",
       " 'PopFmle20-242010',\n",
       " 'PopMale25-292010',\n",
       " 'PopFmle25-292010',\n",
       " 'PopMale30-342010',\n",
       " 'PopFmle30-342010',\n",
       " 'PopMale35-442010',\n",
       " 'PopFmle35-442010',\n",
       " 'PopMale45-542010',\n",
       " 'PopFmle45-542010',\n",
       " 'PopMale55-592010',\n",
       " 'PopFmle55-592010',\n",
       " 'PopMale60-642010',\n",
       " 'PopFmle60-642010',\n",
       " 'PopMale65-742010',\n",
       " 'PopFmle65-742010',\n",
       " 'PopMale75-842010',\n",
       " 'PopFmle75-842010',\n",
       " 'PopMale>842010',\n",
       " 'PopFmle>842010',\n",
       " '3-YrMortalityAge<1Year2015-17',\n",
       " '3-YrMortalityAge1-4Years2015-17',\n",
       " '3-YrMortalityAge5-14Years2015-17',\n",
       " '3-YrMortalityAge15-24Years2015-17',\n",
       " '3-YrMortalityAge25-34Years2015-17',\n",
       " '3-YrMortalityAge35-44Years2015-17',\n",
       " '3-YrMortalityAge45-54Years2015-17',\n",
       " '3-YrMortalityAge55-64Years2015-17',\n",
       " '3-YrMortalityAge65-74Years2015-17',\n",
       " '3-YrMortalityAge75-84Years2015-17',\n",
       " '3-YrMortalityAge85+Years2015-17',\n",
       " 'mortality2015-17Estimated',\n",
       " 'stay at home',\n",
       " '>50 gatherings',\n",
       " '>500 gatherings',\n",
       " 'public schools',\n",
       " 'restaurant dine-in',\n",
       " 'entertainment/gym',\n",
       " 'federal guidelines',\n",
       " 'foreign travel ban',\n",
       " 'SVIPercentile',\n",
       " 'HPSAShortage',\n",
       " 'HPSAServedPop',\n",
       " 'HPSAUnderservedPop',\n",
       " '% Uninsured',\n",
       " '% Vaccinated',\n",
       " 'High School Graduation Rate',\n",
       " '% Unemployed',\n",
       " '% Children in Poverty',\n",
       " '% Single-Parent Households',\n",
       " '% Severe Housing Problems',\n",
       " 'Social Association Rate',\n",
       " 'Mask Never',\n",
       " 'Mask Rarely',\n",
       " 'Mask Sometimes',\n",
       " 'Mask Frequently',\n",
       " 'Mask Always']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Reweighing requires processed numerical data, so we need to pre-process the raw Titanic data before trying to apply it.\n",
    "\n",
    "We'll use a sklearn pipeline to do pre-processing of the data. For those not familiar with sklearn pipelines, they can be used to chain processing, feature engineering and model steps together. This allows for hyperparameters search over for the full pipeline, rather than just the model. They work with sklearn's object orientated fit/transform/predict paradigm.\n",
    "\n",
    "We're not going to use them to their full potential here, but will create custom transformers and a pipeline to do just the initial pre-processing. The transformers will each define .fit() and .transform() methods; .fit() will be used to learn from the training data, and .transform() will be applied to the training and test data.\n",
    "\n",
    "There will be two forks to the pipeline, one to handle object/string features, and the other to handle numeric features. The outputs of these will be combined to create the features for modelling\n",
    "\n",
    "The following cells define classes we need. For convenience, these will work with DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing will be done using a sklearn pipeline. We need these bits to make the transformers and connect them.\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# For the logistic regression model\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectCols(TransformerMixin):\n",
    "    \"\"\"Select columns from a DataFrame.\"\"\"\n",
    "    def __init__(self, cols: List[str]) -> None:\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, x: None) -> \"SelectCols\":\n",
    "        \"\"\"Nothing to do.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Return just selected columns.\"\"\"\n",
    "        return x[self.cols]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sex  Survived\n",
       "731    male         0\n",
       "383  female         1\n",
       "402  female         0\n",
       "696    male         0\n",
       "98   female         1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SelectCols(cols=['Sex', 'Survived'])\n",
    "sc.transform(train.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(TransformerMixin):\n",
    "    \"\"\"Convert non-numeric columns to numeric using label encoding. \n",
    "    Handles unseen data on transform.\"\"\"\n",
    "    def fit(self, x: pd.DataFrame) -> \"LabelEncoder\":\n",
    "        \"\"\"Learn encoder for each column.\"\"\"\n",
    "        encoders = {}\n",
    "        for c in x:\n",
    "            # Make encoder using pd.factorize on unique values, \n",
    "            # then convert to a dictionary\n",
    "            v, k = zip(pd.factorize(x[c].unique()))\n",
    "            encoders[c] = dict(zip(k[0], v[0]))\n",
    "\n",
    "        self.encoders_ = encoders\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, x) -> pd.DataFrame:\n",
    "        \"\"\"For columns in x that have learned encoders, apply encoding.\"\"\"\n",
    "        x = x.copy()\n",
    "        for c in x:\n",
    "            # Ignore new, unseen values\n",
    "            x.loc[~x[c].isin(self.encoders_[c]), c] = np.nan\n",
    "            # Map learned labels\n",
    "            x.loc[:, c] = x[c].map(self.encoders_[c])\n",
    "\n",
    "        # Return without nans\n",
    "        return x.fillna(-2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex\n",
       "284       0    0\n",
       "607       0    0\n",
       "568       1    0\n",
       "872       0    0\n",
       "533       1    1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit_transform(train[['Pclass', 'Sex']].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pclass': {1: 0, 3: 1}, 'Sex': {'male': 0, 'female': 1}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.encoders_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericEncoder(TransformerMixin):\n",
    "    \"\"\"Remove invalid values from numerical columns, replace with median.\"\"\"\n",
    "    def fit(self, x: pd.DataFrame) -> \"NumericEncoder\":\n",
    "        \"\"\"Learn median for every column in x.\"\"\"\n",
    "        # Find median for all columns, handling non-NaNs invalid values and NaNs\n",
    "        # Where all values are NaNs (after coercion) the median value will be a NaN.\n",
    "        self.encoders_ = {\n",
    "            c: pd.to_numeric(x[c],\n",
    "                             errors='coerce').median(skipna=True) for c in x}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"For each column in x, encode NaN values are learned \n",
    "        median and add a flag column indicating where these \n",
    "        replacements were made\"\"\"\n",
    "\n",
    "        # Create a list of new DataFrames, each with 2 columns\n",
    "        output_dfs = []\n",
    "        for c in x:\n",
    "            new_cols = pd.DataFrame()\n",
    "            # Find invalid values that aren't nans (-inf, inf, string)\n",
    "            invalid_idx = pd.to_numeric(x[c].replace([-np.inf, np.inf],\n",
    "                                                     np.nan),\n",
    "                                        errors='coerce').isnull()\n",
    "\n",
    "            # Copy to new df for this column\n",
    "            new_cols.loc[:, c] = x[c].copy()\n",
    "            # Replace the invalid values with learned median\n",
    "            new_cols.loc[invalid_idx, c] = self.encoders_[c]\n",
    "            # Mark these replacement in a new column called \n",
    "            # \"[column_name]_invalid_flag\"\n",
    "            new_cols.loc[:, f\"{c}_invalid_flag\"] = invalid_idx.astype(np.int8)\n",
    "\n",
    "            output_dfs.append(new_cols)\n",
    "\n",
    "        # Concat list of output_dfs to single df\n",
    "        df = pd.concat(output_dfs,\n",
    "                       axis=1)\n",
    "\n",
    "        # Return wtih an remaining NaNs removed. These might exist if the median\n",
    "        # is a NaN because there was no numeric data in the column at all.\n",
    "        return df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_invalid_flag</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Fare_invalid_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.3000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.4958</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Age_invalid_flag     Fare  Fare_invalid_flag\n",
       "624  21.0                 0  16.1000                  0\n",
       "369  24.0                 0  69.3000                  0\n",
       "333  16.0                 0  18.0000                  0\n",
       "200  28.0                 0   9.5000                  0\n",
       "514  24.0                 0   7.4958                  0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne = NumericEncoder()\n",
    "ne.fit_transform(train[['Age', 'Fare']].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': 24.0, 'Fare': 16.1}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne.encoders_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the pipeline\n",
    "\n",
    "The two forks are joined as individual pipelines, then their output is concatenated using a feature union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constructing the pipeline\n",
    "\n",
    "# LabelEncoding fork: Select object columns -> label encode\n",
    "pp_object_cols = Pipeline([('select', SelectCols(cols=['Sex', 'Survived', \n",
    "                                                       'Cabin', 'Ticket', \n",
    "                                                       'SibSp', 'Embarked',\n",
    "                                                       'Parch', 'Pclass',\n",
    "                                                       'Name'])),\n",
    "                           ('process', LabelEncoder())])\n",
    "\n",
    "# NumericEncoding fork: Select numeric columns -> numeric encode\n",
    "pp_numeric_cols = Pipeline([('select', SelectCols(cols=['Age', \n",
    "                                                        'Fare'])),\n",
    "                            ('process', NumericEncoder())])\n",
    "\n",
    "\n",
    "# We won't use the next part, but typically the pipeline would continue to \n",
    "# the model (after dropping 'Survived' from the training data, of course). \n",
    "# For example:\n",
    "pp_pipeline = FeatureUnion([('object_cols', pp_object_cols),\n",
    "                            ('numeric_cols', pp_numeric_cols)])\n",
    "\n",
    "model_pipeline = Pipeline([('pp', pp_pipeline),\n",
    "                           ('mod', LogisticRegression())])\n",
    "# This could be run with model.pipeline.fit_predict(x), and passed to a \n",
    "# gridsearch object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the pipeline\n",
    "\n",
    "pp_pipeline.fit_transform() can called on the training set, however for the sake of sticking with DataFrames, we'll avoid calling the FeatureUnion and do it like this instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_invalid_flag</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Fare_invalid_flag</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.3000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>73.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>135</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Age_invalid_flag     Fare  Fare_invalid_flag  Sex  Survived  Cabin  \\\n",
       "641  24.0                 0  69.3000                  0    0         1      1   \n",
       "120  21.0                 0  73.5000                  0    1         0     -2   \n",
       "866  27.0                 0  13.8583                  0    0         1     -2   \n",
       "425  29.0                 1   7.2500                  0    1         0     -2   \n",
       "594  37.0                 0  26.0000                  0    1         0     -2   \n",
       "\n",
       "     Ticket  SibSp  Embarked  Parch  Pclass  Name  \n",
       "641       9      0         2      0       2    -2  \n",
       "120     135      2         1      0       1    -2  \n",
       "866      -2      1         2      0       1    -2  \n",
       "425      -2      0         1      0       0    -2  \n",
       "594      -2      1         1      0       1    -2  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_, valid = train_test_split(train,\n",
    "                                 test_size=0.3)\n",
    "\n",
    "# .fit_transform on train\n",
    "train_pp = pd.concat((pp_numeric_cols.fit_transform(train_), \n",
    "                      pp_object_cols.fit_transform(train_)),\n",
    "                     axis=1)\n",
    "\n",
    "# .transform on valid\n",
    "valid_pp = pd.concat((pp_numeric_cols.transform(valid), \n",
    "                      pp_object_cols.transform(valid)),\n",
    "                     axis=1)\n",
    "valid_pp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_invalid_flag</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Fare_invalid_flag</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.9000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>189</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.7208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Age_invalid_flag     Fare  Fare_invalid_flag  Sex  Survived  Cabin  \\\n",
       "339  29.0                 1   7.2292                  0    1         0     -2   \n",
       "10   29.0                 1   7.8958                  0    1         0     -2   \n",
       "86   27.0                 0   7.8792                  0    0         0     -2   \n",
       "140  10.0                 0  46.9000                  0    0         0     -2   \n",
       "15   24.0                 0  27.7208                  0    0         0     -2   \n",
       "\n",
       "     Ticket  SibSp  Embarked  Parch  Pclass  Name  \n",
       "339      -2      0         2      0       0    -2  \n",
       "10       -2      0         1      0       0    -2  \n",
       "86       -2      0         0      0       0    -2  \n",
       "140     189      6         1      2       0    -2  \n",
       "15      168      1         2      0       1    -2  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .transform on test\n",
    "test_pp = pd.concat((pp_numeric_cols.transform(test), \n",
    "                     pp_object_cols.transform(test)),\n",
    "                    axis=1)\n",
    "test_pp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark models\n",
    "\n",
    "## Prepare the data\n",
    "Split the features and targets, and create a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Survived'\n",
    "x_columns = [c for c in train_pp if c != target]\n",
    "x_train, y_train = train_pp[x_columns], train_pp[target]\n",
    "x_valid, y_valid = valid_pp[x_columns], valid_pp[target]\n",
    "x_test = test_pp[x_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: Accuracy: 0.7836, Best Params: {'logisticregression__C': 10}\n",
      "SVC: Accuracy: 0.7985, Best Params: {'svc__C': 1, 'svc__kernel': 'rbf'}\n",
      "DecisionTreeClassifier: Accuracy: 0.6978, Best Params: {'decisiontreeclassifier__max_depth': 10}\n",
      "RandomForestClassifier: Accuracy: 0.7799, Best Params: {'randomforestclassifier__max_depth': 10, 'randomforestclassifier__n_estimators': 10}\n",
      "GradientBoostingClassifier: Accuracy: 0.8022, Best Params: {'gradientboostingclassifier__learning_rate': 0.01, 'gradientboostingclassifier__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Exemple de données\n",
    "# X_train, y_train, X_val, y_val = ...\n",
    "\n",
    "models_and_parameters = {\n",
    "    'LogisticRegression': (LogisticRegression(),\n",
    "                           {'logisticregression__C': [0.1, 1, 10]}),\n",
    "    'SVC': (SVC(),\n",
    "            {'svc__C': [0.1, 1, 10], 'svc__kernel': ['linear', 'rbf']}),\n",
    "    'DecisionTreeClassifier': (DecisionTreeClassifier(),\n",
    "                               {'decisiontreeclassifier__max_depth': [None, 10, 20, 30]}),\n",
    "    'RandomForestClassifier': (RandomForestClassifier(),\n",
    "                               {'randomforestclassifier__n_estimators': [10, 50, 100], 'randomforestclassifier__max_depth': [None, 10, 20]}),\n",
    "    'GradientBoostingClassifier': (GradientBoostingClassifier(),\n",
    "                                   {'gradientboostingclassifier__n_estimators': [100, 200], 'gradientboostingclassifier__learning_rate': [0.1, 0.01]})\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, (model, params) in models_and_parameters.items():\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), (model_name.lower(), model)])\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=-1)\n",
    "    \n",
    "    # Formation et optimisation avec X_train, y_train\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    # Évaluation sur X_val, y_val\n",
    "\n",
    "    y_val_pred = grid_search.predict(x_valid)\n",
    "    \n",
    "    val_accuracy = accuracy_score(y_valid, y_val_pred)\n",
    "    \n",
    "    results[model_name] = f'Accuracy: {val_accuracy:.4f}, Best Params: {grid_search.best_params_}'\n",
    "    print(f\"{model_name}: {results[model_name]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models\n",
    "\n",
    "Train LogisticRegression and a RandomForestClassifier. Also dump out the test predictions to see how to do on the leaderboard.\n",
    "\n",
    "warnings is just used here to shut up some FutureWarnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/Users/bigmac/Desktop/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression validation accuracy: 0.8097014925373134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bigmac/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "biased_lr = LogisticRegression()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "    biased_lr.fit(x_train, y_train)\n",
    "    \n",
    "print(f\"Logistic regression validation accuracy: {biased_lr.score(x_valid, y_valid)}\")\n",
    "\n",
    "sub.loc[:, 'Survived'] = biased_lr.predict(x_test).astype(int)\n",
    "sub.to_csv('biased_lr.csv', \n",
    "           index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest validation accuracy: 0.832089552238806\n"
     ]
    }
   ],
   "source": [
    "biased_rfc = RandomForestClassifier(n_estimators=100, \n",
    "                                    max_depth=4)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', FutureWarning)\n",
    "    \n",
    "    biased_rfc.fit(x_train, y_train)\n",
    "    \n",
    "print(f\"Random forest validation accuracy: {biased_rfc.score(x_valid, y_valid)}\")\n",
    "\n",
    "sub.loc[:, 'Survived'] = biased_rfc.predict(x_test).astype(int)\n",
    "sub.to_csv('biased_rfc.csv', \n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n",
      "/var/folders/mj/9ss0rbqx7jg9yrgt8b0lsrcw0000gn/T/ipykernel_73832/1589811173.py:24: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  x.loc[:, c] = x[c].map(self.encoders_[c])\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat((pp_numeric_cols.fit_transform(train), \n",
    "                      pp_object_cols.fit_transform(train)),\n",
    "                     axis=1)\n",
    "\n",
    "test = pd.concat((pp_numeric_cols.fit_transform(test), \n",
    "                      pp_object_cols.fit_transform(test)),\n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_invalid_flag</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Fare_invalid_flag</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.2458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.2417</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.6500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.1083</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>680</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>77.2875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Age_invalid_flag      Fare  Fare_invalid_flag  Sex  Survived  \\\n",
       "709  28.0                 1   15.2458                  0    0         0   \n",
       "439  31.0                 0   10.5000                  0    0         1   \n",
       "840  20.0                 0    7.9250                  0    0         1   \n",
       "720   6.0                 0   33.0000                  0    1         0   \n",
       "39   14.0                 0   11.2417                  0    1         0   \n",
       "..    ...               ...       ...                ...  ...       ...   \n",
       "106  21.0                 0    7.6500                  0    1         0   \n",
       "270  28.0                 1   31.0000                  0    0         1   \n",
       "860  41.0                 0   14.1083                  0    0         1   \n",
       "435  14.0                 0  120.0000                  0    1         0   \n",
       "102  21.0                 0   77.2875                  0    0         1   \n",
       "\n",
       "     Cabin  Ticket  SibSp  Embarked  Parch  Pclass  Name  \n",
       "709     -2       0      0         0      0       0     0  \n",
       "439     -2       1      1         1      1       1     1  \n",
       "840     -2       2      1         1      1       0     2  \n",
       "720     -2       3      1         1      0       1     3  \n",
       "39      -2       4      0         0      1       0     4  \n",
       "..     ...     ...    ...       ...    ...     ...   ...  \n",
       "106     -2     679      1         1      1       0   886  \n",
       "270     -2      37      1         1      1       2   887  \n",
       "860     -2     680      2         1      1       0   888  \n",
       "435      7      51      0         1      2       2   889  \n",
       "102     95     432      1         1      0       2   890  \n",
       "\n",
       "[891 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_invalid_flag</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Fare_invalid_flag</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.1833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>360</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.2417</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>361</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>362</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Age_invalid_flag      Fare  Fare_invalid_flag  Sex  Survived  \\\n",
       "321  25.0                 0    7.2292                  0    0         0   \n",
       "324  39.0                 0  211.3375                  0    1         0   \n",
       "388  21.0                 0    7.7500                  0    0         0   \n",
       "56   35.0                 0    7.8958                  0    0         0   \n",
       "153  36.0                 0   12.1833                  0    1         0   \n",
       "..    ...               ...       ...                ...  ...       ...   \n",
       "71   21.0                 0    7.8958                  0    0         0   \n",
       "106  21.0                 0    7.8208                  0    0         0   \n",
       "270  46.0                 0   75.2417                  0    0         0   \n",
       "348  24.0                 0   13.5000                  0    0         0   \n",
       "102  27.0                 1    7.7500                  0    0         0   \n",
       "\n",
       "     Cabin  Ticket  SibSp  Embarked  Parch  Pclass  Name  \n",
       "321      0       0      0         0      0       0     0  \n",
       "324     -2       1      0         1      0       1     1  \n",
       "388     -2       2      0         2      0       0     2  \n",
       "56      -2       3      0         1      0       0     3  \n",
       "153     -2       4      0         1      1       0     4  \n",
       "..     ...     ...    ...       ...    ...     ...   ...  \n",
       "71      -2     359      0         1      0       0   413  \n",
       "106     -2     360      0         2      0       0   414  \n",
       "270     21     126      0         0      0       1   415  \n",
       "348     -2     361      0         1      0       2   416  \n",
       "102     -2     362      0         2      0       0   417  \n",
       "\n",
       "[418 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Fair Kernel method for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "\n",
    "class  ObliviousData:\n",
    "    def build_O_discrete(self,K,S):\n",
    "        n = math.floor(((np.shape(K))[1])/2) # n = half the samples\n",
    "        self.n = n\n",
    "        self.K=K[:2*n,:2*n] #remove one data point if n is odd\n",
    "        \n",
    "        #Bin S\n",
    "        S_binned = S #assuming that S is discrete and starts at 0\n",
    "        self.S_train = S_binned[0:n]\n",
    "        self.S_cond = S_binned[n:2*n]\n",
    "        \n",
    "        self.S_max = int(max(S_binned)+1) \n",
    "   \n",
    "        #precompute\n",
    "\n",
    "        Ephi = np.mean(K[n:,n:])\n",
    "        \n",
    "        mean_iI = np.zeros((n,self.S_max))\n",
    "        mean_i = np.zeros((n,1))\n",
    "        for i in range(n):\n",
    "            mean_i[i] = np.mean(K[i,n:])\n",
    "            for u in range(self.S_max): \n",
    "                I = n + np.where(self.S_cond==u)[0] # all I in cond which correspond to sensitive value u\n",
    "                mean_iI[i,u] = np.mean(K[i,I])\n",
    "\n",
    "        mean_I = np.zeros((self.S_max,1))\n",
    "        mean_IJ = np.zeros((self.S_max,self.S_max))\n",
    "        for u in range(self.S_max):\n",
    "            I = n + np.where(self.S_cond==u)[0] # all I in cond which correspond to sensitive value u\n",
    "            mean_I[u] = np.mean(K[I,n:])\n",
    "            for v in range(self.S_max):\n",
    "                J = n + np.where(self.S_cond==v)[0] # all I in cond which correspond to sensitive value v\n",
    "                mean_IJ[u,v] = np.mean((K[I,:])[:,J]) \n",
    "\n",
    "\n",
    "        #final loop\n",
    "        O = copy(K[:n,:n]) #Kernel matrix for the first n elements\n",
    "        for i in range(n):\n",
    "            for j in range(i,n):\n",
    "                u = int(self.S_train[i])\n",
    "                v = int(self.S_train[j])\n",
    "    \n",
    "                O[i,j] = O[i,j] - mean_iI[i,v]  - mean_iI[j,u]  + mean_IJ[u,v] + mean_i[j] + mean_i[i] - mean_I[u] - mean_I[v]  + Ephi\n",
    "                O[j,i] = O[i,j]\n",
    "                \n",
    "        self.O = O\n",
    "        self.mean_i = mean_i\n",
    "        self.mean_iI = mean_iI\n",
    "        self.mean_I = mean_I\n",
    "        self.mean_IJ = mean_IJ\n",
    "        self.Ephi = Ephi\n",
    "                \n",
    "        return O\n",
    "    \n",
    "    \n",
    "    def build_Ot_discrete(self,Kt,St): #Kt has size testsample x 2n \n",
    "        Ot = copy(Kt[:,:self.n])\n",
    "        m = (np.shape(Kt))[0]\n",
    "    \n",
    "        mean_test_iI = np.zeros((m,self.S_max))\n",
    "        mean_test_i = np.zeros((m,1))\n",
    "        for i in range(m):\n",
    "            mean_test_i[i] = np.mean(Kt[i,self.n:])\n",
    "            for u in range(self.S_max):\n",
    "                I = self.n + np.where(self.S_cond==u)[0] # all I in cond which correspond to sensitive value u\n",
    "                mean_test_iI[i,u] = np.mean(Kt[i,I])\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(self.n): #not symmetric anymore so we need to go through all\n",
    "                u = int(St[i])\n",
    "                v = int(self.S_train[j]) \n",
    "                Ot[i,j] = Ot[i,j] - mean_test_iI[i,v]- self.mean_iI[j,u]  + self.mean_IJ[u,v]  +  self.mean_i[j] + mean_test_i[i]  - self.mean_I[u] - self.mean_I[v]  + self.Ephi # i corresponds to test, j to train; \n",
    "        self.Ot = Ot\n",
    "        return self.Ot \n",
    "    \n",
    "    \n",
    "    def predict_g(self,Kt, St): #for M-Oblivious \n",
    "        m = (np.shape(Kt))[0]#kt has size testsample x 2n \n",
    "        mean_test_iI = np.zeros((self.n,self.S_max))\n",
    "        mean_test_i = np.zeros((self.n,1))\n",
    "        for i in range(self.n):\n",
    "            mean_test_i[i] = np.mean(self.K[i,self.n:])\n",
    "            for u in range(self.S_max):\n",
    "                I = self.n + np.where(self.S_cond==u)[0] # all i in cond which correspond to sensitive value u\n",
    "                mean_test_iI[i,u] = np.mean(self.K[i,I])\n",
    "        \n",
    "        M_XS = np.matlib.repmat(mean_test_i, 1, self.S_max)-mean_test_iI\n",
    "        gOt=np.zeros((m,self.n))\n",
    "        for i in range(m):\n",
    "            gOt[i,:]=M_XS[:,St[i][0][0]]\n",
    "        \n",
    "        gOt= gOt+Kt[:,:self.n]\n",
    "\n",
    "        return gOt \n",
    "    \n",
    "    \n",
    "\n",
    "    def build_K_rbf(self,X1,X2,sigma=1):\n",
    "        # X1 has n rows = number of samples; X2 has m rows = number of samples \n",
    "        n = (np.shape(X1))[0]\n",
    "        m = (np.shape(X2))[0]\n",
    "        K_1 = np.zeros((n,m))\n",
    "    \n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                K_1[i,j] = np.exp((-1)*(np.dot(X1[i]-X2[j],X1[i]-X2[j]))/sigma)\n",
    "                \n",
    "        return K_1\n",
    "\n",
    "\n",
    "    def build_K_lin(self,X1,X2):\n",
    "        # X1 has n rows = number of samples; X2 has m rows = number of samples \n",
    "        n = (np.shape(X1))[0]\n",
    "        m = (np.shape(X2))[0]\n",
    "        K = np.zeros((n,m))\n",
    "    \n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                K[i,j] = np.dot(X1[i],X2[j])\n",
    "        \n",
    "\n",
    "        return K\n",
    "\n",
    "    \n",
    "    def Omatrix(self):\n",
    "        return self.O\n",
    "    \n",
    "    def Otmatrix(self):\n",
    "        return self.Ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import numpy.matlib\n",
    "from sklearn import svm\n",
    "from collections import namedtuple\n",
    "\n",
    "def generate_truncnorm_samples(n_samples,lower,upper,mu,sigma):\n",
    "    X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "    values = X.rvs(n_samples)\n",
    "    return values\n",
    "\n",
    "def generate_toy_data(n_samples):\n",
    "    sigma =0.5\n",
    "    unique_sensitive_feature_values=[0,1]\n",
    "    max_non_sensitive_feature_value=4.0\n",
    "    min_non_sensitive_feature_value=1.0\n",
    "    mu = 0.5*(max_non_sensitive_feature_value+min_non_sensitive_feature_value)\n",
    "    sensitive_features = [unique_sensitive_feature_values[0]] * n_samples + [unique_sensitive_feature_values[1]] * n_samples \n",
    "    sensitive_features = np.array(sensitive_features)\n",
    "    sensitive_features.shape = (len(sensitive_features), 1)\n",
    "    \n",
    "    Lower = generate_truncnorm_samples(n_samples,min_non_sensitive_feature_value,max_non_sensitive_feature_value,mu,sigma)\n",
    "    Upper = generate_truncnorm_samples(n_samples,min_non_sensitive_feature_value,max_non_sensitive_feature_value,mu,sigma)\n",
    "    non_sensitive_features0=[Lower]+[Upper]\n",
    "    non_sensitive_features0 = np.array(np.hstack(non_sensitive_features0))\n",
    "    non_sensitive_features0.shape=(len(non_sensitive_features0),1)\n",
    "    \n",
    "    \n",
    "    non_sensitive_features=[Lower-stats.bernoulli(0.9).rvs(n_samples)*1]+[Upper+stats.bernoulli(0.9).rvs(n_samples)*1]\n",
    "    non_sensitive_features = np.array(np.hstack(non_sensitive_features))\n",
    "    non_sensitive_features.shape=(len(non_sensitive_features),1)\n",
    "    \n",
    "    X = np.hstack([non_sensitive_features, sensitive_features])\n",
    "    \n",
    "    threshold=mu\n",
    "    Y_Bernoulli_params=np.array(non_sensitive_features0/max_non_sensitive_feature_value).flatten()\n",
    "    Y=np.array([stats.bernoulli(Y_Bernoulli_params[i]).rvs(1) for i in range(len(Y_Bernoulli_params))]).flatten()\n",
    "    True_Y=Y*(np.array((non_sensitive_features0>=threshold)*1).flatten())\n",
    "    Y=Y*(np.array((non_sensitive_features+sensitive_features>=threshold)*1).flatten())\n",
    "    sensitive_feature_id=np.shape(X)[1]-1\n",
    "\n",
    "    return X, Y, sensitive_feature_id, True_Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def estimate_beta_dependence(predicted_labels, sensitive_features, labels):\n",
    "    estimated_beta=0\n",
    "    n = np.size(predicted_labels)\n",
    "    unique_label_freqs =[]\n",
    "    for i in range(len(labels)):\n",
    "        unique_label_freqs.append(np.mean(predicted_labels==labels[i]))\n",
    "\n",
    "    unique_S_features=list(set(sensitive_features))\n",
    "    unique_S_freqs =[]\n",
    "    for i in range(len(unique_S_features)):\n",
    "        unique_S_freqs.append(np.mean(sensitive_features==unique_S_features[i]))\n",
    "\n",
    "    pred_feature_pairs = np.vstack((predicted_labels, sensitive_features)).T\n",
    "     \n",
    "    joint_freqs=[]       \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(unique_S_features)):\n",
    "            pattern=(labels[i],unique_S_features[j])\n",
    "            joint_freq=np.size(np.where(np.sum(np.abs(pred_feature_pairs-np.matlib.repmat(pattern, n, 1)),axis=1)==0))/n\n",
    "            joint_freqs.append(joint_freq)\n",
    "            marginal_label_freq=unique_label_freqs[i]\n",
    "            marginal_S_freq=unique_S_freqs[j]\n",
    "            estimated_beta=estimated_beta+np.abs(marginal_label_freq * marginal_S_freq-joint_freq)\n",
    "    return estimated_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_invalid_flag</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Fare_invalid_flag</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.2458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.2417</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.6500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.1083</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>680</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>77.2875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Age_invalid_flag      Fare  Fare_invalid_flag  Sex  Survived  \\\n",
       "709  28.0                 1   15.2458                  0    0         0   \n",
       "439  31.0                 0   10.5000                  0    0         1   \n",
       "840  20.0                 0    7.9250                  0    0         1   \n",
       "720   6.0                 0   33.0000                  0    1         0   \n",
       "39   14.0                 0   11.2417                  0    1         0   \n",
       "..    ...               ...       ...                ...  ...       ...   \n",
       "106  21.0                 0    7.6500                  0    1         0   \n",
       "270  28.0                 1   31.0000                  0    0         1   \n",
       "860  41.0                 0   14.1083                  0    0         1   \n",
       "435  14.0                 0  120.0000                  0    1         0   \n",
       "102  21.0                 0   77.2875                  0    0         1   \n",
       "\n",
       "     Cabin  Ticket  SibSp  Embarked  Parch  Pclass  Name  \n",
       "709     -2       0      0         0      0       0     0  \n",
       "439     -2       1      1         1      1       1     1  \n",
       "840     -2       2      1         1      1       0     2  \n",
       "720     -2       3      1         1      0       1     3  \n",
       "39      -2       4      0         0      1       0     4  \n",
       "..     ...     ...    ...       ...    ...     ...   ...  \n",
       "106     -2     679      1         1      1       0   886  \n",
       "270     -2      37      1         1      1       2   887  \n",
       "860     -2     680      2         1      1       0   888  \n",
       "435      7      51      0         1      2       2   889  \n",
       "102     95     432      1         1      0       2   890  \n",
       "\n",
       "[891 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = train[:445].drop('Survived', axis=1), train[:445][\"Survived\"]\n",
    "X_cond,y_cond = train[445:].drop('Survived', axis=1), train[445:][\"Survived\"]\n",
    "X_test,y_test = test.drop('Survived', axis=1), test[\"Survived\"]\n",
    "\n",
    "\n",
    "X = pd.concat((X_train, X_cond))\n",
    "y = np.concatenate((y_train,y_cond))\n",
    "S_train = X_train['Pclass']\n",
    "S_train_cond = X_cond['Pclass']\n",
    "S = np.concatenate((S_train, S_train_cond))\n",
    "S_test = X_test['Pclass']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_invalid_flag</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Fare_invalid_flag</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.2458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.2417</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>380</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.9500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.5875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>382</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>445 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Age_invalid_flag     Fare  Fare_invalid_flag  Sex  Cabin  Ticket  \\\n",
       "709  28.0                 1  15.2458                  0    0     -2       0   \n",
       "439  31.0                 0  10.5000                  0    0     -2       1   \n",
       "840  20.0                 0   7.9250                  0    0     -2       2   \n",
       "720   6.0                 0  33.0000                  0    1     -2       3   \n",
       "39   14.0                 0  11.2417                  0    1     -2       4   \n",
       "..    ...               ...      ...                ...  ...    ...     ...   \n",
       "68   17.0                 0   7.9250                  0    1     -2     380   \n",
       "825  28.0                 1   6.9500                  0    0     -2     381   \n",
       "188  40.0                 0  15.5000                  0    0     -2     149   \n",
       "271  25.0                 0   0.0000                  0    0     -2     221   \n",
       "662  47.0                 0  25.5875                  0    0     83     382   \n",
       "\n",
       "     SibSp  Embarked  Parch  Pclass  Name  \n",
       "709      0         0      0       0     0  \n",
       "439      1         1      1       1     1  \n",
       "840      1         1      1       0     2  \n",
       "720      1         1      0       1     3  \n",
       "39       0         0      1       0     4  \n",
       "..     ...       ...    ...     ...   ...  \n",
       "68       4         1      2       0   440  \n",
       "825      1         2      1       0   441  \n",
       "188      0         2      0       0   442  \n",
       "271      1         1      1       0   443  \n",
       "662      1         1      1       2   444  \n",
       "\n",
       "[445 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "709    0\n",
       "439    1\n",
       "840    1\n",
       "720    0\n",
       "39     0\n",
       "      ..\n",
       "68     0\n",
       "825    1\n",
       "188    1\n",
       "271    0\n",
       "662    1\n",
       "Name: Survived, Length: 445, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.count_nonzero(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_invalid_flag</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Fare_invalid_flag</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.1833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>360</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.2417</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>361</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>362</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Age_invalid_flag      Fare  Fare_invalid_flag  Sex  Cabin  Ticket  \\\n",
       "321  25.0                 0    7.2292                  0    0      0       0   \n",
       "324  39.0                 0  211.3375                  0    1     -2       1   \n",
       "388  21.0                 0    7.7500                  0    0     -2       2   \n",
       "56   35.0                 0    7.8958                  0    0     -2       3   \n",
       "153  36.0                 0   12.1833                  0    1     -2       4   \n",
       "..    ...               ...       ...                ...  ...    ...     ...   \n",
       "71   21.0                 0    7.8958                  0    0     -2     359   \n",
       "106  21.0                 0    7.8208                  0    0     -2     360   \n",
       "270  46.0                 0   75.2417                  0    0     21     126   \n",
       "348  24.0                 0   13.5000                  0    0     -2     361   \n",
       "102  27.0                 1    7.7500                  0    0     -2     362   \n",
       "\n",
       "     SibSp  Embarked  Parch  Pclass  Name  \n",
       "321      0         0      0       0     0  \n",
       "324      0         1      0       1     1  \n",
       "388      0         2      0       0     2  \n",
       "56       0         1      0       0     3  \n",
       "153      0         1      1       0     4  \n",
       "..     ...       ...    ...     ...   ...  \n",
       "71       0         1      0       0   413  \n",
       "106      0         2      0       0   414  \n",
       "270      0         0      0       1   415  \n",
       "348      0         1      0       2   416  \n",
       "102      0         2      0       0   417  \n",
       "\n",
       "[418 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_labels=list(set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot data\n",
    "plt.hist(X_test['Pclass'])\n",
    "sensitive_feature_values = sorted(list(set(X['Pclass'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321    0\n",
       "324    0\n",
       "388    0\n",
       "56     0\n",
       "153    0\n",
       "      ..\n",
       "71     0\n",
       "106    0\n",
       "270    0\n",
       "348    0\n",
       "102    0\n",
       "Name: Survived, Length: 418, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.count_nonzero(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,   0.,   0.,   0., 418.,   0.,   0.,   0.,   0.]),\n",
       " array([-0.5, -0.4, -0.3, -0.2, -0.1,  0. ,  0.1,  0.2,  0.3,  0.4,  0.5]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBLIVIOUS SVM\n",
      "C:  0.0625\n",
      "Fitting OBLIVIOUS SVM\n",
      "Standard Missclassification acc:  0.36363636363636365\n",
      "Beta-Dependence:  0.039582427142235754\n",
      "C:  0.125\n",
      "Fitting OBLIVIOUS SVM\n",
      "Standard Missclassification acc:  0.361244019138756\n",
      "Beta-Dependence:  0.049506650488770815\n",
      "C:  0.25\n",
      "Fitting OBLIVIOUS SVM\n",
      "Standard Missclassification acc:  0.361244019138756\n",
      "Beta-Dependence:  0.049506650488770815\n",
      "C:  0.5\n",
      "Fitting OBLIVIOUS SVM\n",
      "Standard Missclassification acc:  0.361244019138756\n",
      "Beta-Dependence:  0.049506650488770815\n",
      "C:  1\n",
      "Fitting OBLIVIOUS SVM\n",
      "Standard Missclassification acc:  0.3588516746411483\n",
      "Beta-Dependence:  0.045946750303335504\n",
      "C:  2\n",
      "Fitting OBLIVIOUS SVM\n",
      "Standard Missclassification acc:  0.34688995215311\n",
      "Beta-Dependence:  0.03293193837137426\n",
      "C:  4\n",
      "Fitting OBLIVIOUS SVM\n",
      "Standard Missclassification acc:  0.39952153110047844\n",
      "Beta-Dependence:  0.08538037132849519\n",
      "C:  8\n",
      "Fitting OBLIVIOUS SVM\n",
      "Standard Missclassification acc:  0.4354066985645933\n",
      "Beta-Dependence:  0.19859893317460675\n",
      "C:  16\n",
      "Fitting OBLIVIOUS SVM\n",
      "Standard Missclassification acc:  0.4354066985645933\n",
      "Beta-Dependence:  0.23209175614111394\n"
     ]
    }
   ],
   "source": [
    "C_list = [2**v for v in range(16,20)]    \n",
    "#Oblivious SVM\n",
    "print('OBLIVIOUS SVM')\n",
    "obl = ObliviousData()\n",
    "K = obl.build_K_lin(np.array(X),np.array(X))\n",
    "O = obl.build_O_discrete(K,S)\n",
    "Kt = obl.build_K_lin(np.array(X_test),np.array(X))\n",
    "Ot = obl.build_Ot_discrete(Kt,np.array(S_test))\n",
    "\n",
    "stdacc_list,true_acc_list,betas =[],[],[]\n",
    "    \n",
    "for c in C_list:\n",
    "    print('C: ', c)\n",
    "    clf = svm.SVC(kernel='precomputed', C=c)\n",
    "    print('Fitting OBLIVIOUS SVM')\n",
    "    clf.fit(O,y_train)\n",
    "    pred_oblv = clf.predict(Ot)\n",
    "    standard_missclassification_error=np.mean((pred_oblv==y_test)*1) # error with respect to observed labels \n",
    "    print('Standard Missclassification acc: ', standard_missclassification_error)\n",
    "    # Dependence measure\n",
    "    beta = estimate_beta_dependence(pred_oblv,X_test['Pclass'],unique_labels)\n",
    "    print('Beta-Dependence: ',beta)\n",
    "    stdacc_list.append(standard_missclassification_error)\n",
    "    betas.append(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(standard error, dependence):\n",
      "(0.45454545454545453, 0.2140060896041757)\n"
     ]
    }
   ],
   "source": [
    "stdind=np.where(stdacc_list==np.max(stdacc_list))[0][0]\n",
    "print('(standard error, dependence):')\n",
    "print((stdacc_list[stdind], betas[stdind]))        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
