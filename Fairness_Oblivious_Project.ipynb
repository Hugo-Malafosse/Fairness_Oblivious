{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fainess project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical Expenditure Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3.](#Table-of-Contents) Training models on original 2015 Panel 19 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Datasets\n",
    "from aif360.datasets import MEPSDataset19\n",
    "from aif360.datasets import MEPSDataset20\n",
    "from aif360.datasets import MEPSDataset21\n",
    "\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Explainers\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Bias mitigation techniques\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "\n",
    "# LIME\n",
    "from aif360.datasets.lime_encoder import LimeEncoder\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load data & create splits for learning/validating/testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "\n",
    "# Graphs libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "from plotly import tools\n",
    "\n",
    "# Libraries to study\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import LFR, Reweighing\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing, RejectOptionClassification\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "\n",
    "# Design libraries\n",
    "from IPython.display import Markdown, display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataset and split into train (50%), validate (30%), and test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset_orig_panel19_train,\n",
    " dataset_orig_panel19_val,\n",
    " dataset_orig_panel19_test) = MEPSDataset19().split([0.5, 0.8], shuffle=True)\n",
    "\n",
    "sens_ind = 0\n",
    "sens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\n",
    "\n",
    "unprivileged_groups = [{sens_attr: v} for v in\n",
    "                       dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\n",
    "privileged_groups = [{sens_attr: v} for v in\n",
    "                     dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_panel19_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used throughout the notebook to print out some labels, names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def describe(train=None, val=None, test=None):\n",
    "    if train is not None:\n",
    "        display(Markdown(\"#### Training Dataset shape\"))\n",
    "        print(train.features.shape)\n",
    "    if val is not None:\n",
    "        display(Markdown(\"#### Validation Dataset shape\"))\n",
    "        print(val.features.shape)\n",
    "    display(Markdown(\"#### Test Dataset shape\"))\n",
    "    print(test.features.shape)\n",
    "    display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "    print(test.favorable_label, test.unfavorable_label)\n",
    "    display(Markdown(\"#### Protected attribute names\"))\n",
    "    print(test.protected_attribute_names)\n",
    "    display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "    print(test.privileged_protected_attributes, \n",
    "          test.unprivileged_protected_attributes)\n",
    "    display(Markdown(\"#### Dataset feature names\"))\n",
    "    print(test.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show 2015 dataset details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(dataset_orig_panel19_train, dataset_orig_panel19_val, dataset_orig_panel19_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_orig_panel19_train = BinaryLabelDatasetMetric(\n",
    "        dataset_orig_panel19_train,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "explainer_orig_panel19_train = MetricTextExplainer(metric_orig_panel19_train)\n",
    "\n",
    "print(explainer_orig_panel19_train.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the models have some good metrics (not excellent), so I will use these models for origin models.\n",
    "\n",
    "## <a id='4'>4. Bias and Fairness</a>\n",
    "\n",
    "Today, a problem of the model that can be produce by Machine Learning is bias that data can have. So a question is how to measure those bias and how to avoid them. In python there is a package produced by IBM called [aif360](https://github.com/IBM/AIF360) that can gives us some metrics and algorithms to know if our data / model are bias and to get a fair model.\n",
    "\n",
    "### <a id='4.1'>4.1 Metrics</a>\n",
    "\n",
    "So with aif360 we have some metrics that indicate if our data or model are bias. I will use 5 metrics : \n",
    "* Statistical Parity Difference\n",
    "* Equal Opportunity Difference\n",
    "* Average Absolute Odds Difference\n",
    "* Disparate Impact\n",
    "* Theil Index\n",
    "\n",
    "#### <a id='4.1.1'>4.1.1 Statistical Parity Difference</a>\n",
    "\n",
    "This measure is based on the following formula : \n",
    "\n",
    "$$ Pr(Y=1|D=unprivileged) - Pr(Y=1|D=privileged) $$\n",
    "\n",
    "Here the bias or *statistical imparity* is the difference between the probability that a random individual drawn from unprivileged is labeled 1 (so here that he has more than 50K for income) and the probability that a random individual from privileged is labeled 1.\n",
    "\n",
    "So it has to be close to **0** so it will be fair.\n",
    "\n",
    "Also you can find more details about that here : [One definition of algorithmic fairness: statistical parity](https://jeremykun.com/2015/10/19/one-definition-of-algorithmic-fairness-statistical-parity/)\n",
    "\n",
    "\n",
    "#### <a id='4.1.2'>4.1.2 Equal Opportunity Difference</a>\n",
    "\n",
    "This metric is just a difference between the true positive rate of unprivileged group and the true positive rate of privileged group so it follows this formula :\n",
    "\n",
    "$$ TPR_{D=unprivileged} - TPR_{D=privileged} $$ \n",
    "\n",
    "Same as the previous metric we need it to be close to **0**.\n",
    "\n",
    "#### <a id='4.1.3'>4.1.3 Average Absolute Odds Difference</a>\n",
    "\n",
    "This measure is using both false positive rate and true positive rate to calculate the bias. It's calculating the equality of odds with the next formula :\n",
    "\n",
    "$$ \\frac{1}{2}[|FPR_{D=unprivileged} - FPR_{D=privileged} | + | TPR_{D=unprivileged} - TPR_{D=privileged}|]$$\n",
    "\n",
    "It needs to be equal to **0** to be fair.\n",
    "\n",
    "#### <a id='4.1.4'>4.1.4 Disparate Impact</a>\n",
    "\n",
    "For this metric we use the following formula :\n",
    "\n",
    "$$ \\frac{Pr(Y=1|D=unprivileged)}{Pr(Y=1|D=privileged)} $$\n",
    "\n",
    "Like the first metric we use both probabities of a random individual drawn from unprivileged or privileged with a label of 1 but here it's a ratio. \n",
    "\n",
    "It changes the objective, for the disparate impact it's **1** that we need.\n",
    "\n",
    "#### <a id='4.1.5'>4.1.5 Theil Index</a>\n",
    "\n",
    "This measure is also known as the generalized entropy index but with $\\alpha$ equals to 1 (more informations on [the Wikipedia page](https://en.wikipedia.org/wiki/Generalized_entropy_index)). So we can calculate it with this formula :\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=0}^{n} \\frac{b_i}{\\mu} ln \\frac{b_i}{\\mu} $$ \n",
    "\n",
    "Where $b_i = \\hat{y}_i - y_i + 1 $\n",
    "\n",
    "So it needs to be close to **0** to be fair.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Learning a Logistic Regression (LR) classifier on original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Training LR model on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_orig_panel19_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      LogisticRegression(solver='liblinear', random_state=1))\n",
    "fit_params = {'logisticregression__sample_weight': dataset.instance_weights}\n",
    "\n",
    "lr_orig_panel19 = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Validating LR model on original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used throughout the tutorial to find best threshold using a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def test(dataset, model, thresh_arr):\n",
    "    try:\n",
    "        # sklearn classifier\n",
    "        y_val_pred_prob = model.predict_proba(dataset.features)\n",
    "        pos_ind = np.where(model.classes_ == dataset.favorable_label)[0][0]\n",
    "    except AttributeError:\n",
    "        # aif360 inprocessing algorithm\n",
    "        y_val_pred_prob = model.predict(dataset).scores\n",
    "        pos_ind = 0\n",
    "    \n",
    "    metric_arrs = defaultdict(list)\n",
    "    for thresh in thresh_arr:\n",
    "        y_val_pred = (y_val_pred_prob[:, pos_ind] > thresh).astype(np.float64)\n",
    "\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = y_val_pred\n",
    "        metric = ClassificationMetric(\n",
    "                dataset, dataset_pred,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "        metric_arrs['bal_acc'].append((metric.true_positive_rate()\n",
    "                                     + metric.true_negative_rate()) / 2)\n",
    "        metric_arrs['avg_odds_diff'].append(metric.average_odds_difference())\n",
    "        metric_arrs['disp_imp'].append(metric.disparate_impact())\n",
    "        metric_arrs['stat_par_diff'].append(metric.statistical_parity_difference())\n",
    "        metric_arrs['eq_opp_diff'].append(metric.equal_opportunity_difference())\n",
    "        metric_arrs['theil_ind'].append(metric.theil_index())\n",
    "    \n",
    "    return metric_arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics = test(dataset=dataset_orig_panel19_val,\n",
    "                   model=lr_orig_panel19,\n",
    "                   thresh_arr=thresh_arr)\n",
    "lr_orig_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function to print out accuracy and fairness metrics. This will be used throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_metrics(metrics, thresh_arr):\n",
    "    best_ind = np.argmax(metrics['bal_acc'])\n",
    "    print(\"Threshold corresponding to Best balanced accuracy: {:6.4f}\".format(thresh_arr[best_ind]))\n",
    "    print(\"Best balanced accuracy: {:6.4f}\".format(metrics['bal_acc'][best_ind]))\n",
    "#     disp_imp_at_best_ind = np.abs(1 - np.array(metrics['disp_imp']))[best_ind]\n",
    "    disp_imp_at_best_ind = 1 - min(metrics['disp_imp'][best_ind], 1/metrics['disp_imp'][best_ind])\n",
    "    print(\"Corresponding 1-min(DI, 1/DI) value: {:6.4f}\".format(disp_imp_at_best_ind))\n",
    "    print(\"Corresponding average odds difference value: {:6.4f}\".format(metrics['avg_odds_diff'][best_ind]))\n",
    "    print(\"Corresponding statistical parity difference value: {:6.4f}\".format(metrics['stat_par_diff'][best_ind]))\n",
    "    print(\"Corresponding equal opportunity difference value: {:6.4f}\".format(metrics['eq_opp_diff'][best_ind]))\n",
    "    print(\"Corresponding Theil index value: {:6.4f}\".format(metrics['theil_ind'][best_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. Testing LR model on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_orig_metrics = test(dataset=dataset_orig_panel19_test,\n",
    "                       model=lr_orig_panel19,\n",
    "                       thresh_arr=[thresh_arr[lr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(lr_orig_metrics, [thresh_arr[lr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Learning a Random Forest (RF) classifier on original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. Training RF model on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_orig_panel19_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      RandomForestClassifier(n_estimators=500, min_samples_leaf=25))\n",
    "fit_params = {'randomforestclassifier__sample_weight': dataset.instance_weights}\n",
    "rf_orig_panel19 = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Validating RF model on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics = test(dataset=dataset_orig_panel19_val,\n",
    "                   model=rf_orig_panel19,\n",
    "                   thresh_arr=thresh_arr)\n",
    "rf_orig_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3. Testing RF model on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_orig_metrics = test(dataset=dataset_orig_panel19_test,\n",
    "                       model=rf_orig_panel19,\n",
    "                       thresh_arr=[thresh_arr[rf_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(rf_orig_metrics, [thresh_arr[rf_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case of the logistic regression classifier learned on the original data, the fairness metrics for the random forest classifier have values that are quite far from 0.\n",
    "\n",
    "For example, 1 - min(DI, 1/DI) has a value of over 0.5 as opposed to the desired value of < 0.2.\n",
    "\n",
    "This indicates that the random forest classifier learned on the original data is also unfair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.](#Table-of-Contents) Bias mitigation using pre-processing technique - Reweighing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_panel19_train = RW.fit_transform(dataset_orig_panel19_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_transf_panel19_train = BinaryLabelDatasetMetric(\n",
    "        dataset_transf_panel19_train,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "explainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\n",
    "\n",
    "print(explainer_transf_panel19_train.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Learning a Logistic Regression (LR) classifier on data transformed by reweighing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Training LR model after reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_transf_panel19_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      LogisticRegression(solver='liblinear', random_state=1))\n",
    "fit_params = {'logisticregression__sample_weight': dataset.instance_weights}\n",
    "lr_transf_panel19 = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Validating  LR model after reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics = test(dataset=dataset_orig_panel19_val,\n",
    "                   model=lr_transf_panel19,\n",
    "                   thresh_arr=thresh_arr)\n",
    "lr_transf_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Testing  LR model after reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_transf_metrics = test(dataset=dataset_orig_panel19_test,\n",
    "                         model=lr_transf_panel19,\n",
    "                         thresh_arr=[thresh_arr[lr_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(lr_transf_metrics, [thresh_arr[lr_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fairness metrics for the logistic regression model learned after reweighing are well improved, and thus the model is much more fair relative to the logistic regression model learned from the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Learning a Random Forest (RF) classifier on data transformed by reweighing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1. Training  RF model after reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_transf_panel19_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      RandomForestClassifier(n_estimators=500, min_samples_leaf=25))\n",
    "fit_params = {'randomforestclassifier__sample_weight': dataset.instance_weights}\n",
    "rf_transf_panel19 = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2. Validating  RF model after reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics = test(dataset=dataset_orig_panel19_val,\n",
    "                   model=rf_transf_panel19,\n",
    "                   thresh_arr=thresh_arr)\n",
    "rf_transf_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3. Testing  RF model after reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_transf_metrics = test(dataset=dataset_orig_panel19_test,\n",
    "                         model=rf_transf_panel19,\n",
    "                         thresh_arr=[thresh_arr[rf_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(rf_transf_metrics, [thresh_arr[rf_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the model learned from the transformed data is fairer than that learned from the original data. However, the random forest model learned from the transformed data is still relatively unfair as compared to the logistic regression model learned from the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Fairness metrics and model evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(data, selected_attr=None):\n",
    "    unprivileged_groups = []\n",
    "    privileged_groups = []\n",
    "    if selected_attr == None:\n",
    "        selected_attr = data.protected_attribute_names\n",
    "    \n",
    "    for attr in selected_attr:\n",
    "            idx = data.protected_attribute_names.index(attr)\n",
    "            privileged_groups.append({attr:data.privileged_protected_attributes[idx]}) \n",
    "            unprivileged_groups.append({attr:data.unprivileged_protected_attributes[idx]}) \n",
    "\n",
    "    return privileged_groups, unprivileged_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
    "\n",
    "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
    "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_metrics(dataset, pred, pred_is_dataset=False):\n",
    "    if pred_is_dataset:\n",
    "        dataset_pred = pred\n",
    "    else:\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = pred\n",
    "    \n",
    "    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference',  'disparate_impact', 'theil_index']\n",
    "    obj_fairness = [[0,0,0,1,0]]\n",
    "    \n",
    "    fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)\n",
    "    \n",
    "    for attr in dataset_pred.protected_attribute_names:\n",
    "        idx = dataset_pred.protected_attribute_names.index(attr)\n",
    "        privileged_groups =  [{attr:dataset_pred.privileged_protected_attributes[idx][0]}] \n",
    "        unprivileged_groups = [{attr:dataset_pred.unprivileged_protected_attributes[idx][0]}] \n",
    "        \n",
    "        classified_metric = ClassificationMetric(dataset, \n",
    "                                                     dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        metric_pred = BinaryLabelDatasetMetric(dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        acc = classified_metric.accuracy()\n",
    "\n",
    "        row = pd.DataFrame([[metric_pred.mean_difference(),\n",
    "                                classified_metric.equal_opportunity_difference(),\n",
    "                                classified_metric.average_abs_odds_difference(),\n",
    "                                metric_pred.disparate_impact(),\n",
    "                                classified_metric.theil_index()]],\n",
    "                           columns  = cols,\n",
    "                           index = [attr]\n",
    "                          )\n",
    "        fair_metrics = fair_metrics.append(row)    \n",
    "    \n",
    "    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)\n",
    "        \n",
    "    return fair_metrics\n",
    "\n",
    "def plot_fair_metrics(fair_metrics):\n",
    "    fig, ax = plt.subplots(figsize=(20,4), ncols=5, nrows=1)\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "        left    =  0.125, \n",
    "        bottom  =  0.1, \n",
    "        right   =  0.9, \n",
    "        top     =  0.9, \n",
    "        wspace  =  .5, \n",
    "        hspace  =  1.1\n",
    "    )\n",
    "\n",
    "    y_title_margin = 1.2\n",
    "\n",
    "    plt.suptitle(\"Fairness metrics\", y = 1.09, fontsize=20)\n",
    "    sns.set(style=\"dark\")\n",
    "\n",
    "    cols = fair_metrics.columns.values\n",
    "    obj = fair_metrics.loc['objective']\n",
    "    size_rect = [0.2,0.2,0.2,0.4,0.25]\n",
    "    rect = [-0.1,-0.1,-0.1,0.8,0]\n",
    "    bottom = [-1,-1,-1,0,0]\n",
    "    top = [1,1,1,2,1]\n",
    "    bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.2],[0,0.25]]\n",
    "\n",
    "    display(Markdown(\"### Check bias metrics :\"))\n",
    "    display(Markdown(\"A model can be considered bias if just one of these five metrics show that this model is biased.\"))\n",
    "    for attr in fair_metrics.index[1:len(fair_metrics)].values:\n",
    "        display(Markdown(\"#### For the %s attribute :\"%attr))\n",
    "        check = [bound[i][0] < fair_metrics.loc[attr][i] < bound[i][1] for i in range(0,5)]\n",
    "        display(Markdown(\"With default thresholds, bias against unprivileged group detected in **%d** out of 5 metrics\"%(5 - sum(check))))\n",
    "\n",
    "    for i in range(0,5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])\n",
    "        \n",
    "        for j in range(0,len(fair_metrics)-1):\n",
    "            a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]\n",
    "            marg = -0.2 if val < 0 else 0.1\n",
    "            ax.text(a.get_x()+a.get_width()/5, a.get_y()+a.get_height()+marg, round(val, 3), fontsize=15,color='black')\n",
    "\n",
    "        plt.ylim(bottom[i], top[i])\n",
    "        plt.setp(ax.patches, linewidth=0)\n",
    "        ax.add_patch(patches.Rectangle((-5,rect[i]), 10, size_rect[i], alpha=0.3, facecolor=\"green\", linewidth=1, linestyle='solid'))\n",
    "        plt.axhline(obj[i], color='black', alpha=0.3)\n",
    "        plt.title(cols[i])\n",
    "        ax.set_ylabel('')    \n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fair_metrics_and_plot(data, model, plot=True, model_aif=False):\n",
    "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
    "    # fair_metrics function available in the metrics.py file\n",
    "    fair = fair_metrics(data, pred)\n",
    "\n",
    "    if plot:\n",
    "        # plot_fair_metrics function available in the visualisations.py file\n",
    "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
    "        plot_fair_metrics(fair)\n",
    "        display(fair)\n",
    "    \n",
    "    return fair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "##### <a id='4.2.1.1'>4.2.1.1 Disparate impact remover</a>\n",
    "Source : [Feldman et al., 2015](https://dl.acm.org/citation.cfm?doid=2783258.2783311)\n",
    "\n",
    "Disparate impact remover is a preprocessing technique that edits feature values increase group fairness while preserving rank-ordering within groups.\n",
    "If you want to see how it works you can take a look on [an example Notebook from the GitHub of AIF360](https://github.com/IBM/AIF360/blob/master/examples/demo_disparate_impact_remover.ipynb).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig_train, data_orig_test = dataset_orig_panel19_train, dataset_orig_panel19_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "DIR = DisparateImpactRemover()\n",
    "data_transf_train = DIR.fit_transform(dataset_orig_panel19_train)\n",
    "\n",
    "rf_transf = RandomForestClassifier().fit(data_transf_train.features, \n",
    "                     data_transf_train.labels.ravel(), \n",
    "                     sample_weight=data_transf_train.instance_weights)\n",
    "\n",
    "data_transf_test = DIR.fit_transform(dataset_orig_panel19_test)\n",
    "fair = get_fair_metrics_and_plot(data_transf_test, rf_transf, plot=False)\n",
    "probs = rf_transf.predict_proba(dataset_orig_panel19_test.features)\n",
    "preds = rf_transf.predict(dataset_orig_panel19_test.features)\n",
    "\n",
    "\n",
    "\n",
    "algo_metrics = add_to_df_algo_metrics(algo_metrics, rf_transf, fair, preds, probs, 'DisparateImpact')\n",
    "\n",
    "y_val_pred = rf_transf.predict(dataset_orig_panel19_val.features)\n",
    "dataset_pred = dataset_orig_panel19_val.copy()\n",
    "dataset_pred.labels = y_val_pred\n",
    "\n",
    "metric = ClassificationMetric(\n",
    "                dataset_orig_panel19_val, dataset_pred,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "acc1 = np.mean( rf_transf.predict(dataset_orig_panel19_val.features) == dataset_orig_panel19_val.labels.ravel())\n",
    "acc = (metric.true_positive_rate()+ metric.true_negative_rate()) / 2\n",
    "\n",
    "print('accuracy score :: ', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "##### <a id='4.2.1.2'>4.2.1.2 Learning fair representations</a>\n",
    "Source : [Zemel et al., 2013](http://proceedings.mlr.press/v28/zemel13.html)\n",
    "\n",
    "Learning fair representations is a pre-processing technique that finds a latent representation which encodes the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(data_transf_test.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups, unprivileged_groups = get_attributes(dataset_orig_panel19_train, selected_attr=['RACE'])\n",
    "\n",
    "\n",
    "LFR_model = LFR(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups, k=1, verbose=0)\n",
    "# LFR.fit(data_orig_train)\n",
    "data_transf_train = LFR_model.fit_transform(dataset_orig_panel19_train)\n",
    "\n",
    "# Train and save the model\n",
    "rf_transf = RandomForestClassifier().fit(data_transf_train.features, \n",
    "                     data_transf_train.labels.ravel(), \n",
    "                     sample_weight=data_transf_train.instance_weights)\n",
    "\n",
    "data_transf_test = LFR_model.transform(dataset_orig_panel19_test)\n",
    "fair = get_fair_metrics_and_plot(data_transf_test, rf_transf, plot=False)\n",
    "probs = rf_transf.predict_proba(dataset_orig_panel19_test.features)\n",
    "preds = rf_transf.predict(dataset_orig_panel19_test.features)\n",
    "\n",
    "algo_metrics = add_to_df_algo_metrics(algo_metrics, rf_transf, fair, preds, probs, 'LFR')\n",
    "\n",
    "data_transf_test\n",
    "\n",
    "y_val_pred = rf_transf.predict(dataset_orig_panel19_val.features)\n",
    "dataset_pred = dataset_orig_panel19_val.copy()\n",
    "dataset_pred.labels = y_val_pred\n",
    "\n",
    "metric = ClassificationMetric(\n",
    "                dataset_orig_panel19_val, dataset_pred,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "acc1 = np.mean( rf_transf.predict(dataset_orig_panel19_val.features) == dataset_orig_panel19_val.labels.ravel())\n",
    "acc = (metric.true_positive_rate()+ metric.true_negative_rate()) / 2\n",
    "\n",
    "print('accuracy score :: ', acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "##### <a id='4.2.1.3'>4.2.1.3 Optimized preprocessing</a>\n",
    "Source : [Calmon et al., 2017](http://papers.nips.cc/paper/6988-optimized-pre-processing-for-discrimination-prevention)\n",
    "\n",
    "Optimized preprocessing is a preprocessing technique that learns a probabilistic transformation that edits the features and labels in the data with group fairness, individual distortion, and data fidelity constraints and objectives.\n",
    "There is also [a demo notebook on the aif360 GitHub](https://github.com/IBM/AIF360/blob/master/examples/demo_optim_data_preproc.ipynb).\n",
    "\n",
    "*To be honest I tried to work with this one but it's more complicated : it uses options that you have to configure yourself and I don't really find how to choose it. Also it use an Optimizer and I didn't find how to build this class. (I didn't read the paper about this algorithm)*\n",
    "*****\n",
    "##### <a id='4.2.1.4'>4.2.1.4 Reweighing</a>\n",
    "Source : [Kamiran and Calders, 2012](https://link.springer.com/article/10.1007%2Fs10115-011-0463-8)\n",
    "\n",
    "Reweighing is a preprocessing technique that Weights the examples in each (group, label) combination differently to ensure fairness before classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups, unprivileged_groups = get_attributes(data_orig_train, selected_attr=['RACE'])\n",
    "t0 = time()\n",
    "\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "# RW.fit(data_orig_train)\n",
    "data_transf_train = RW.fit_transform(data_orig_train)\n",
    "\n",
    "# Train and save the model\n",
    "rf_transf = RandomForestClassifier().fit(data_transf_train.features, \n",
    "                     data_transf_train.labels.ravel(), \n",
    "                     sample_weight=data_transf_train.instance_weights)\n",
    "\n",
    "data_transf_test = RW.transform(data_orig_test)\n",
    "fair = get_fair_metrics_and_plot(data_orig_test, rf_transf, plot=False)\n",
    "probs = rf_transf.predict_proba(data_orig_test.features)\n",
    "preds = rf_transf.predict(data_orig_test.features)\n",
    "\n",
    "\n",
    "algo_metrics = add_to_df_algo_metrics(algo_metrics, rf_transf, fair, preds, probs, 'Reweighing')\n",
    "\n",
    "y_val_pred = rf_transf.predict(dataset_orig_panel19_val.features)\n",
    "dataset_pred = dataset_orig_panel19_val.copy()\n",
    "dataset_pred.labels = y_val_pred\n",
    "\n",
    "metric = ClassificationMetric(\n",
    "                dataset_orig_panel19_val, dataset_pred,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "acc1 = np.mean( rf_transf.predict(dataset_orig_panel19_val.features) == dataset_orig_panel19_val.labels.ravel())\n",
    "acc = (metric.true_positive_rate()+ metric.true_negative_rate()) / 2\n",
    "\n",
    "print('accuracy score :: ', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(algo_metrics)):\n",
    "    print(\"model name :: \", algo_metrics.index[i])\n",
    "    display(\"fairness metrics :: \", algo_metrics[\"fair_metrics\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## [5.](#Table-of-Contents) Bias mitigation using in-processing technique - Prejudice Remover (PR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Learning a Prejudice Remover (PR) model on original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1. Training a PR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PrejudiceRemover(sensitive_attr=sens_attr, eta=25.0)\n",
    "pr_orig_scaler = StandardScaler()\n",
    "\n",
    "dataset = dataset_orig_panel19_train.copy()\n",
    "dataset.features = pr_orig_scaler.fit_transform(dataset.features)\n",
    "\n",
    "pr_orig_panel19 = model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2. Validating PR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.50, 50)\n",
    "\n",
    "dataset = dataset_orig_panel19_val.copy()\n",
    "dataset.features = pr_orig_scaler.transform(dataset.features)\n",
    "\n",
    "val_metrics = test(dataset=dataset,\n",
    "                   model=pr_orig_panel19,\n",
    "                   thresh_arr=thresh_arr)\n",
    "pr_orig_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3. Testing PR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_orig_panel19_test.copy()\n",
    "dataset.features = pr_orig_scaler.transform(dataset.features)\n",
    "\n",
    "pr_orig_metrics = test(dataset=dataset,\n",
    "                       model=pr_orig_panel19,\n",
    "                       thresh_arr=[thresh_arr[pr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_metrics(pr_orig_metrics, [thresh_arr[pr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case of reweighing, prejudice remover results in a fair model. However, it has come at the expense of relatively lower balanced accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6.](#Table-of-Contents) Summary of Model Learning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.multi_sparse', False)\n",
    "results = [lr_orig_metrics, rf_orig_metrics, lr_transf_metrics,\n",
    "           rf_transf_metrics, pr_orig_metrics]\n",
    "debias = pd.Series(['']*2 + ['Reweighing']*2\n",
    "                 + ['Prejudice Remover'],\n",
    "                   name='Bias Mitigator')\n",
    "clf = pd.Series(['Logistic Regression', 'Random Forest']*2 + [''],\n",
    "                name='Classifier')\n",
    "pd.concat([pd.DataFrame(metrics) for metrics in results], axis=0).set_index([debias, clf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the models, the logistic regression model gives the best balance in terms of balanced accuracy and fairness. While the model learnt by prejudice remover is slightly fairer, it has much lower accuracy. All other models are quite unfair compared to the logistic model. Hence, we take the logistic regression model learnt from data transformed by re-weighing and 'deploy' it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [7.](#Table-of-Contents) Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "\n",
    "class  ObliviousData:\n",
    "    def build_O_discrete(self,K,S):\n",
    "        n = math.floor(((np.shape(K))[1])/2) # n = half the samples\n",
    "        self.n = n\n",
    "        self.K=K[:2*n,:2*n] #remove one data point if n is odd\n",
    "        \n",
    "        #Bin S\n",
    "        S_binned = S #assuming that S is discrete and starts at 0\n",
    "        self.S_train = S_binned[0:n]\n",
    "        self.S_cond = S_binned[n:2*n]\n",
    "        \n",
    "        self.S_max = int(max(S_binned)+1) \n",
    "   \n",
    "        #precompute\n",
    "\n",
    "        Ephi = np.mean(K[n:,n:])\n",
    "        \n",
    "        mean_iI = np.zeros((n,self.S_max))\n",
    "        mean_i = np.zeros((n,1))\n",
    "        for i in range(n):\n",
    "            mean_i[i] = np.mean(K[i,n:])\n",
    "            for u in range(self.S_max): \n",
    "                I = n + np.where(self.S_cond==u)[0] # all I in cond which correspond to sensitive value u\n",
    "                mean_iI[i,u] = np.mean(K[i,I])\n",
    "\n",
    "        mean_I = np.zeros((self.S_max,1))\n",
    "        mean_IJ = np.zeros((self.S_max,self.S_max))\n",
    "        for u in range(self.S_max):\n",
    "            I = n + np.where(self.S_cond==u)[0] # all I in cond which correspond to sensitive value u\n",
    "            mean_I[u] = np.mean(K[I,n:])\n",
    "            for v in range(self.S_max):\n",
    "                J = n + np.where(self.S_cond==v)[0] # all I in cond which correspond to sensitive value v\n",
    "                mean_IJ[u,v] = np.mean((K[I,:])[:,J]) \n",
    "\n",
    "\n",
    "        #final loop\n",
    "        O = copy(K[:n,:n]) #Kernel matrix for the first n elements\n",
    "        for i in range(n):\n",
    "            for j in range(i,n):\n",
    "                u = int(self.S_train[i])\n",
    "                v = int(self.S_train[j])\n",
    "    \n",
    "                O[i,j] = O[i,j] - mean_iI[i,v]  - mean_iI[j,u]  + mean_IJ[u,v] + mean_i[j] + mean_i[i] - mean_I[u] - mean_I[v]  + Ephi\n",
    "                O[j,i] = O[i,j]\n",
    "                \n",
    "        self.O = O\n",
    "        self.mean_i = mean_i\n",
    "        self.mean_iI = mean_iI\n",
    "        self.mean_I = mean_I\n",
    "        self.mean_IJ = mean_IJ\n",
    "        self.Ephi = Ephi\n",
    "                \n",
    "        return O\n",
    "    \n",
    "    \n",
    "    def build_Ot_discrete(self,Kt,St): #Kt has size testsample x 2n \n",
    "        Ot = copy(Kt[:,:self.n])\n",
    "        m = (np.shape(Kt))[0]\n",
    "    \n",
    "        mean_test_iI = np.zeros((m,self.S_max))\n",
    "        mean_test_i = np.zeros((m,1))\n",
    "        for i in range(m):\n",
    "            mean_test_i[i] = np.mean(Kt[i,self.n:])\n",
    "            for u in range(self.S_max):\n",
    "                I = self.n + np.where(self.S_cond==u)[0] # all I in cond which correspond to sensitive value u\n",
    "                mean_test_iI[i,u] = np.mean(Kt[i,I])\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(self.n): #not symmetric anymore so we need to go through all\n",
    "                u = int(St[i])\n",
    "                v = int(self.S_train[j]) \n",
    "                Ot[i,j] = Ot[i,j] - mean_test_iI[i,v]- self.mean_iI[j,u]  + self.mean_IJ[u,v]  +  self.mean_i[j] + mean_test_i[i]  - self.mean_I[u] - self.mean_I[v]  + self.Ephi # i corresponds to test, j to train; \n",
    "        self.Ot = Ot\n",
    "        return self.Ot \n",
    "    \n",
    "    \n",
    "    def predict_g(self,Kt, St): #for M-Oblivious \n",
    "        m = (np.shape(Kt))[0]#kt has size testsample x 2n \n",
    "        mean_test_iI = np.zeros((self.n,self.S_max))\n",
    "        mean_test_i = np.zeros((self.n,1))\n",
    "        for i in range(self.n):\n",
    "            mean_test_i[i] = np.mean(self.K[i,self.n:])\n",
    "            for u in range(self.S_max):\n",
    "                I = self.n + np.where(self.S_cond==u)[0] # all i in cond which correspond to sensitive value u\n",
    "                mean_test_iI[i,u] = np.mean(self.K[i,I])\n",
    "        \n",
    "        M_XS = np.matlib.repmat(mean_test_i, 1, self.S_max)-mean_test_iI\n",
    "        gOt=np.zeros((m,self.n))\n",
    "        for i in range(m):\n",
    "            gOt[i,:]=M_XS[:,St[i][0][0]]\n",
    "        \n",
    "        gOt= gOt+Kt[:,:self.n]\n",
    "\n",
    "        return gOt \n",
    "    \n",
    "    \n",
    "\n",
    "    def build_K_rbf(self,X1,X2,sigma=1):\n",
    "        # X1 has n rows = number of samples; X2 has m rows = number of samples \n",
    "        n = (np.shape(X1))[0]\n",
    "        m = (np.shape(X2))[0]\n",
    "        K_1 = np.zeros((n,m))\n",
    "    \n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                K_1[i,j] = np.exp((-1)*(np.dot(X1[i]-X2[j],X1[i]-X2[j]))/sigma)\n",
    "                \n",
    "        return K_1\n",
    "\n",
    "\n",
    "    def build_K_lin(self,X1,X2):\n",
    "        # X1 has n rows = number of samples; X2 has m rows = number of samples \n",
    "        n = (np.shape(X1))[0]\n",
    "        m = (np.shape(X2))[0]\n",
    "        K = np.zeros((n,m))\n",
    "    \n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                K[i,j] = np.dot(X1[i],X2[j])\n",
    "        \n",
    "\n",
    "        return K\n",
    "\n",
    "    \n",
    "    def Omatrix(self):\n",
    "        return self.O\n",
    "    \n",
    "    def Otmatrix(self):\n",
    "        return self.Ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import numpy.matlib\n",
    "from sklearn import svm\n",
    "from collections import namedtuple\n",
    "\n",
    "def generate_truncnorm_samples(n_samples,lower,upper,mu,sigma):\n",
    "    X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "    values = X.rvs(n_samples)\n",
    "    return values\n",
    "\n",
    "def generate_toy_data(n_samples):\n",
    "    sigma =0.5\n",
    "    unique_sensitive_feature_values=[0,1]\n",
    "    max_non_sensitive_feature_value=4.0\n",
    "    min_non_sensitive_feature_value=1.0\n",
    "    mu = 0.5*(max_non_sensitive_feature_value+min_non_sensitive_feature_value)\n",
    "    sensitive_features = [unique_sensitive_feature_values[0]] * n_samples + [unique_sensitive_feature_values[1]] * n_samples \n",
    "    sensitive_features = np.array(sensitive_features)\n",
    "    sensitive_features.shape = (len(sensitive_features), 1)\n",
    "    \n",
    "    Lower = generate_truncnorm_samples(n_samples,min_non_sensitive_feature_value,max_non_sensitive_feature_value,mu,sigma)\n",
    "    Upper = generate_truncnorm_samples(n_samples,min_non_sensitive_feature_value,max_non_sensitive_feature_value,mu,sigma)\n",
    "    non_sensitive_features0=[Lower]+[Upper]\n",
    "    non_sensitive_features0 = np.array(np.hstack(non_sensitive_features0))\n",
    "    non_sensitive_features0.shape=(len(non_sensitive_features0),1)\n",
    "    \n",
    "    \n",
    "    non_sensitive_features=[Lower-stats.bernoulli(0.9).rvs(n_samples)*1]+[Upper+stats.bernoulli(0.9).rvs(n_samples)*1]\n",
    "    non_sensitive_features = np.array(np.hstack(non_sensitive_features))\n",
    "    non_sensitive_features.shape=(len(non_sensitive_features),1)\n",
    "    \n",
    "    X = np.hstack([non_sensitive_features, sensitive_features])\n",
    "    \n",
    "    threshold=mu\n",
    "    Y_Bernoulli_params=np.array(non_sensitive_features0/max_non_sensitive_feature_value).flatten()\n",
    "    Y=np.array([stats.bernoulli(Y_Bernoulli_params[i]).rvs(1) for i in range(len(Y_Bernoulli_params))]).flatten()\n",
    "    True_Y=Y*(np.array((non_sensitive_features0>=threshold)*1).flatten())\n",
    "    Y=Y*(np.array((non_sensitive_features+sensitive_features>=threshold)*1).flatten())\n",
    "    sensitive_feature_id=np.shape(X)[1]-1\n",
    "\n",
    "    return X, Y, sensitive_feature_id, True_Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def estimate_beta_dependence(predicted_labels, sensitive_features, labels):\n",
    "    estimated_beta=0\n",
    "    n = np.size(predicted_labels)\n",
    "    unique_label_freqs =[]\n",
    "    for i in range(len(labels)):\n",
    "        unique_label_freqs.append(np.mean(predicted_labels==labels[i]))\n",
    "\n",
    "    unique_S_features=list(set(sensitive_features))\n",
    "    unique_S_freqs =[]\n",
    "    for i in range(len(unique_S_features)):\n",
    "        unique_S_freqs.append(np.mean(sensitive_features==unique_S_features[i]))\n",
    "\n",
    "    pred_feature_pairs = np.vstack((predicted_labels, sensitive_features)).T\n",
    "     \n",
    "    joint_freqs=[]       \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(unique_S_features)):\n",
    "            pattern=(labels[i],unique_S_features[j])\n",
    "            joint_freq=np.size(np.where(np.sum(np.abs(pred_feature_pairs-np.matlib.repmat(pattern, n, 1)),axis=1)==0))/n\n",
    "            joint_freqs.append(joint_freq)\n",
    "            marginal_label_freq=unique_label_freqs[i]\n",
    "            marginal_S_freq=unique_S_freqs[j]\n",
    "            estimated_beta=estimated_beta+np.abs(marginal_label_freq * marginal_S_freq-joint_freq)\n",
    "    return estimated_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig_train.features = data_orig_train.features[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = data_orig_train.features[:3957], data_orig_train.labels[:3957].ravel()\n",
    "X_cond,y_cond = data_orig_train.features[3957:], data_orig_train.labels[3957:].ravel()\n",
    "X_test,y_test = dataset_orig_panel19_val.features, dataset_orig_panel19_val.labels.ravel()\n",
    "\n",
    "\n",
    "\n",
    "X = data_orig_train.features\n",
    "y = data_orig_train.labels.ravel()\n",
    "S_train = data_orig_train.protected_attributes[:3957].ravel()\n",
    "S_train_cond = data_orig_train.protected_attributes[3957:].ravel()\n",
    "S = data_orig_train.protected_attributes.ravel()\n",
    "S_test = dataset_orig_panel19_val.protected_attributes.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels=list(set(list(y_test)))\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot data\n",
    "plt.hist(S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [2**v for v in [0,1,2,3,4,5,6,7,8,9,10]]    \n",
    "#Oblivious SVM\n",
    "print('OBLIVIOUS SVM')\n",
    "obl = ObliviousData()\n",
    "K = obl.build_K_lin(np.array(X),np.array(X))\n",
    "O = obl.build_O_discrete(K,S)\n",
    "Kt = obl.build_K_lin(np.array(X_test),np.array(X))\n",
    "Ot = obl.build_Ot_discrete(Kt,np.array(S_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stdacc_list,true_acc_list,betas =[],[],[]\n",
    "    \n",
    "for c in C_list:\n",
    "    print('C: ', c)\n",
    "    clf = svm.SVC(kernel='precomputed', C=c)\n",
    "    print('Fitting OBLIVIOUS SVM')\n",
    "    clf.fit(O,y_train)\n",
    "    \n",
    "    pred_oblv = clf.predict(Ot)\n",
    "\n",
    "    y_val_pred = pred_oblv\n",
    "    dataset_pred = dataset_orig_panel19_val.copy()\n",
    "    dataset_pred.labels = y_val_pred\n",
    "\n",
    "    metric = ClassificationMetric(\n",
    "                    dataset_orig_panel19_val, dataset_pred,\n",
    "                    unprivileged_groups=unprivileged_groups,\n",
    "                    privileged_groups=privileged_groups)\n",
    "\n",
    "    acc1 = np.mean( rf_transf.predict(dataset_orig_panel19_val.features) == dataset_orig_panel19_val.labels.ravel())\n",
    "    acc = (metric.true_positive_rate()+ metric.true_negative_rate()) / 2\n",
    "\n",
    "    print('accuracy score :: ', acc)\n",
    "    \n",
    "    standard_missclassification_error=np.mean((pred_oblv==y_test)*1) # error with respect to observed labels \n",
    "    print('Standard Missclassification acc: ', standard_missclassification_error)\n",
    "    # Dependence measure\n",
    "    #beta = estimate_beta_dependence(pred_oblv,X_test['Pclass'],unique_labels)\n",
    "    #print('Beta-Dependence: ',beta)\n",
    "    stdacc_list.append(standard_missclassification_error)\n",
    "    #betas.append(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Exemple de donnes\n",
    "# X_train, y_train, X_val, y_val = ...\n",
    "\n",
    "models_and_parameters = {\n",
    "    'LogisticRegression': (LogisticRegression(),\n",
    "                           {'logisticregression__C': [0.1, 1, 10]}),\n",
    "    'SVC': (SVC(),\n",
    "            {'svc__C': [0.1, 1, 10], 'svc__kernel': ['linear', 'rbf']}),\n",
    "    'DecisionTreeClassifier': (DecisionTreeClassifier(),\n",
    "                               {'decisiontreeclassifier__max_depth': [None, 10, 20, 30]}),\n",
    "    'RandomForestClassifier': (RandomForestClassifier(),\n",
    "                               {'randomforestclassifier__n_estimators': [10, 50, 100], 'randomforestclassifier__max_depth': [None, 10, 20]}),\n",
    "    'GradientBoostingClassifier': (GradientBoostingClassifier(),\n",
    "                                   {'gradientboostingclassifier__n_estimators': [100, 200], 'gradientboostingclassifier__learning_rate': [0.1, 0.01]})\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, (model, params) in models_and_parameters.items():\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), (model_name.lower(), model)])\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=-1)\n",
    "    \n",
    "    # Formation et optimisation avec X_train, y_train\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # valuation sur X_val, y_val\n",
    "\n",
    "\n",
    "    val_predictions = grid_search.predict(X_test)\n",
    "    val_accuracy = accuracy_score(y_test, val_predictions)\n",
    "    \n",
    "    results[model_name] = f'Accuracy: {val_accuracy:.4f}, Best Params: {grid_search.best_params_}'\n",
    "    print(f\"{model_name}: {results[model_name]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Titanic dataset\n",
    "\n",
    " Titanic models typically rely too heavily on sex as a feature, which is unacceptable in our modern and enlightened world. To try and address this problem we'll;\n",
    " - Assess the bias in the Titainc dataset\n",
    " - Apply reweighing in pre-processing using the new [IBM AIF 360 toolbox]\n",
    " - Train benchmark and \"fair\" models, using both linear (logistic regression) and non-parametric (random forest) models\n",
    " - Compare the performance of the models\n",
    " - Try different techniques for comparing the fairness of the benchmark and \"fair\" models, and compare the effectiveness of the reweighing approach between logistic regression and random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to use type hinting\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "# Modelling. Warnings will be used to silence various model warnings for tidier output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "\n",
    "# Data handling/display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "\n",
    "# IBM's fairness tooolbox:\n",
    "from aif360.datasets import BinaryLabelDataset  # To handle the data\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric  # For calculating metrics\n",
    "from aif360.explainers import MetricTextExplainer  # For explaining metrics\n",
    "from aif360.algorithms.preprocessing import Reweighing  # Preprocessing technique\n",
    "\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('/Users/bigmac/Desktop/train.csv').sample(frac=1, random_state=42)\n",
    "test = pd.read_csv('/Users/bigmac/Desktop/test.csv').sample(frac=1, random_state=42)\n",
    "test.loc[:, 'Survived'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_ = pd.read_csv('/Users/bigmac/Desktop/county_data_abridged.csv').sample(frac=1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Reweighing requires processed numerical data, so we need to pre-process the raw Titanic data before trying to apply it.\n",
    "\n",
    "We'll use a sklearn pipeline to do pre-processing of the data. For those not familiar with sklearn pipelines, they can be used to chain processing, feature engineering and model steps together. This allows for hyperparameters search over for the full pipeline, rather than just the model. They work with sklearn's object orientated fit/transform/predict paradigm.\n",
    "\n",
    "We're not going to use them to their full potential here, but will create custom transformers and a pipeline to do just the initial pre-processing. The transformers will each define .fit() and .transform() methods; .fit() will be used to learn from the training data, and .transform() will be applied to the training and test data.\n",
    "\n",
    "There will be two forks to the pipeline, one to handle object/string features, and the other to handle numeric features. The outputs of these will be combined to create the features for modelling\n",
    "\n",
    "The following cells define classes we need. For convenience, these will work with DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing will be done using a sklearn pipeline. We need these bits to make the transformers and connect them.\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# For the logistic regression model\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectCols(TransformerMixin):\n",
    "    \"\"\"Select columns from a DataFrame.\"\"\"\n",
    "    def __init__(self, cols: List[str]) -> None:\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, x: None) -> \"SelectCols\":\n",
    "        \"\"\"Nothing to do.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Return just selected columns.\"\"\"\n",
    "        return x[self.cols]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SelectCols(cols=['Sex', 'Survived'])\n",
    "sc.transform(train.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(TransformerMixin):\n",
    "    \"\"\"Convert non-numeric columns to numeric using label encoding. \n",
    "    Handles unseen data on transform.\"\"\"\n",
    "    def fit(self, x: pd.DataFrame) -> \"LabelEncoder\":\n",
    "        \"\"\"Learn encoder for each column.\"\"\"\n",
    "        encoders = {}\n",
    "        for c in x:\n",
    "            # Make encoder using pd.factorize on unique values, \n",
    "            # then convert to a dictionary\n",
    "            v, k = zip(pd.factorize(x[c].unique()))\n",
    "            encoders[c] = dict(zip(k[0], v[0]))\n",
    "\n",
    "        self.encoders_ = encoders\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, x) -> pd.DataFrame:\n",
    "        \"\"\"For columns in x that have learned encoders, apply encoding.\"\"\"\n",
    "        x = x.copy()\n",
    "        for c in x:\n",
    "            # Ignore new, unseen values\n",
    "            x.loc[~x[c].isin(self.encoders_[c]), c] = np.nan\n",
    "            # Map learned labels\n",
    "            x.loc[:, c] = x[c].map(self.encoders_[c])\n",
    "\n",
    "        # Return without nans\n",
    "        return x.fillna(-2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit_transform(train[['Pclass', 'Sex']].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.encoders_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericEncoder(TransformerMixin):\n",
    "    \"\"\"Remove invalid values from numerical columns, replace with median.\"\"\"\n",
    "    def fit(self, x: pd.DataFrame) -> \"NumericEncoder\":\n",
    "        \"\"\"Learn median for every column in x.\"\"\"\n",
    "        # Find median for all columns, handling non-NaNs invalid values and NaNs\n",
    "        # Where all values are NaNs (after coercion) the median value will be a NaN.\n",
    "        self.encoders_ = {\n",
    "            c: pd.to_numeric(x[c],\n",
    "                             errors='coerce').median(skipna=True) for c in x}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"For each column in x, encode NaN values are learned \n",
    "        median and add a flag column indicating where these \n",
    "        replacements were made\"\"\"\n",
    "\n",
    "        # Create a list of new DataFrames, each with 2 columns\n",
    "        output_dfs = []\n",
    "        for c in x:\n",
    "            new_cols = pd.DataFrame()\n",
    "            # Find invalid values that aren't nans (-inf, inf, string)\n",
    "            invalid_idx = pd.to_numeric(x[c].replace([-np.inf, np.inf],\n",
    "                                                     np.nan),\n",
    "                                        errors='coerce').isnull()\n",
    "\n",
    "            # Copy to new df for this column\n",
    "            new_cols.loc[:, c] = x[c].copy()\n",
    "            # Replace the invalid values with learned median\n",
    "            new_cols.loc[invalid_idx, c] = self.encoders_[c]\n",
    "            # Mark these replacement in a new column called \n",
    "            # \"[column_name]_invalid_flag\"\n",
    "            new_cols.loc[:, f\"{c}_invalid_flag\"] = invalid_idx.astype(np.int8)\n",
    "\n",
    "            output_dfs.append(new_cols)\n",
    "\n",
    "        # Concat list of output_dfs to single df\n",
    "        df = pd.concat(output_dfs,\n",
    "                       axis=1)\n",
    "\n",
    "        # Return wtih an remaining NaNs removed. These might exist if the median\n",
    "        # is a NaN because there was no numeric data in the column at all.\n",
    "        return df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne = NumericEncoder()\n",
    "ne.fit_transform(train[['Age', 'Fare']].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne.encoders_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the pipeline\n",
    "\n",
    "The two forks are joined as individual pipelines, then their output is concatenated using a feature union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constructing the pipeline\n",
    "\n",
    "# LabelEncoding fork: Select object columns -> label encode\n",
    "pp_object_cols = Pipeline([('select', SelectCols(cols=['Sex', 'Survived', \n",
    "                                                       'Cabin', 'Ticket', \n",
    "                                                       'SibSp', 'Embarked',\n",
    "                                                       'Parch', 'Pclass',\n",
    "                                                       'Name'])),\n",
    "                           ('process', LabelEncoder())])\n",
    "\n",
    "# NumericEncoding fork: Select numeric columns -> numeric encode\n",
    "pp_numeric_cols = Pipeline([('select', SelectCols(cols=['Age', \n",
    "                                                        'Fare'])),\n",
    "                            ('process', NumericEncoder())])\n",
    "\n",
    "\n",
    "# We won't use the next part, but typically the pipeline would continue to \n",
    "# the model (after dropping 'Survived' from the training data, of course). \n",
    "# For example:\n",
    "pp_pipeline = FeatureUnion([('object_cols', pp_object_cols),\n",
    "                            ('numeric_cols', pp_numeric_cols)])\n",
    "\n",
    "model_pipeline = Pipeline([('pp', pp_pipeline),\n",
    "                           ('mod', LogisticRegression())])\n",
    "# This could be run with model.pipeline.fit_predict(x), and passed to a \n",
    "# gridsearch object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the pipeline\n",
    "\n",
    "pp_pipeline.fit_transform() can called on the training set, however for the sake of sticking with DataFrames, we'll avoid calling the FeatureUnion and do it like this instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_, valid = train_test_split(train,\n",
    "                                 test_size=0.3)\n",
    "\n",
    "# .fit_transform on train\n",
    "train_pp = pd.concat((pp_numeric_cols.fit_transform(train_), \n",
    "                      pp_object_cols.fit_transform(train_)),\n",
    "                     axis=1)\n",
    "\n",
    "# .transform on valid\n",
    "valid_pp = pd.concat((pp_numeric_cols.transform(valid), \n",
    "                      pp_object_cols.transform(valid)),\n",
    "                     axis=1)\n",
    "valid_pp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .transform on test\n",
    "test_pp = pd.concat((pp_numeric_cols.transform(test), \n",
    "                     pp_object_cols.transform(test)),\n",
    "                    axis=1)\n",
    "test_pp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark models\n",
    "\n",
    "## Prepare the data\n",
    "Split the features and targets, and create a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Survived'\n",
    "x_columns = [c for c in train_pp if c != target]\n",
    "x_train, y_train = train_pp[x_columns], train_pp[target]\n",
    "x_valid, y_valid = valid_pp[x_columns], valid_pp[target]\n",
    "x_test = test_pp[x_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Exemple de donnes\n",
    "# X_train, y_train, X_val, y_val = ...\n",
    "\n",
    "models_and_parameters = {\n",
    "    'LogisticRegression': (LogisticRegression(),\n",
    "                           {'logisticregression__C': [0.1, 1, 10]}),\n",
    "    'SVC': (SVC(),\n",
    "            {'svc__C': [0.1, 1, 10], 'svc__kernel': ['linear', 'rbf']}),\n",
    "    'DecisionTreeClassifier': (DecisionTreeClassifier(),\n",
    "                               {'decisiontreeclassifier__max_depth': [None, 10, 20, 30]}),\n",
    "    'RandomForestClassifier': (RandomForestClassifier(),\n",
    "                               {'randomforestclassifier__n_estimators': [10, 50, 100], 'randomforestclassifier__max_depth': [None, 10, 20]}),\n",
    "    'GradientBoostingClassifier': (GradientBoostingClassifier(),\n",
    "                                   {'gradientboostingclassifier__n_estimators': [100, 200], 'gradientboostingclassifier__learning_rate': [0.1, 0.01]})\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, (model, params) in models_and_parameters.items():\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), (model_name.lower(), model)])\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=-1)\n",
    "    \n",
    "    # Formation et optimisation avec X_train, y_train\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    # valuation sur X_val, y_val\n",
    "\n",
    "    y_val_pred = grid_search.predict(x_valid)\n",
    "    \n",
    "    val_accuracy = accuracy_score(y_valid, y_val_pred)\n",
    "    \n",
    "    results[model_name] = f'Accuracy: {val_accuracy:.4f}, Best Params: {grid_search.best_params_}'\n",
    "    print(f\"{model_name}: {results[model_name]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models\n",
    "\n",
    "Train LogisticRegression and a RandomForestClassifier. Also dump out the test predictions to see how to do on the leaderboard.\n",
    "\n",
    "warnings is just used here to shut up some FutureWarnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/Users/bigmac/Desktop/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_lr = LogisticRegression()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "    biased_lr.fit(x_train, y_train)\n",
    "    \n",
    "print(f\"Logistic regression validation accuracy: {biased_lr.score(x_valid, y_valid)}\")\n",
    "\n",
    "sub.loc[:, 'Survived'] = biased_lr.predict(x_test).astype(int)\n",
    "sub.to_csv('biased_lr.csv', \n",
    "           index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_rfc = RandomForestClassifier(n_estimators=100, \n",
    "                                    max_depth=4)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', FutureWarning)\n",
    "    \n",
    "    biased_rfc.fit(x_train, y_train)\n",
    "    \n",
    "print(f\"Random forest validation accuracy: {biased_rfc.score(x_valid, y_valid)}\")\n",
    "\n",
    "sub.loc[:, 'Survived'] = biased_rfc.predict(x_test).astype(int)\n",
    "sub.to_csv('biased_rfc.csv', \n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat((pp_numeric_cols.fit_transform(train), \n",
    "                      pp_object_cols.fit_transform(train)),\n",
    "                     axis=1)\n",
    "\n",
    "test = pd.concat((pp_numeric_cols.fit_transform(test), \n",
    "                      pp_object_cols.fit_transform(test)),\n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Fair Kernel method for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "\n",
    "class  ObliviousData:\n",
    "    def build_O_discrete(self,K,S):\n",
    "        n = math.floor(((np.shape(K))[1])/2) # n = half the samples\n",
    "        self.n = n\n",
    "        self.K=K[:2*n,:2*n] #remove one data point if n is odd\n",
    "        \n",
    "        #Bin S\n",
    "        S_binned = S #assuming that S is discrete and starts at 0\n",
    "        self.S_train = S_binned[0:n]\n",
    "        self.S_cond = S_binned[n:2*n]\n",
    "        \n",
    "        self.S_max = int(max(S_binned)+1) \n",
    "   \n",
    "        #precompute\n",
    "\n",
    "        Ephi = np.mean(K[n:,n:])\n",
    "        \n",
    "        mean_iI = np.zeros((n,self.S_max))\n",
    "        mean_i = np.zeros((n,1))\n",
    "        for i in range(n):\n",
    "            mean_i[i] = np.mean(K[i,n:])\n",
    "            for u in range(self.S_max): \n",
    "                I = n + np.where(self.S_cond==u)[0] # all I in cond which correspond to sensitive value u\n",
    "                mean_iI[i,u] = np.mean(K[i,I])\n",
    "\n",
    "        mean_I = np.zeros((self.S_max,1))\n",
    "        mean_IJ = np.zeros((self.S_max,self.S_max))\n",
    "        for u in range(self.S_max):\n",
    "            I = n + np.where(self.S_cond==u)[0] # all I in cond which correspond to sensitive value u\n",
    "            mean_I[u] = np.mean(K[I,n:])\n",
    "            for v in range(self.S_max):\n",
    "                J = n + np.where(self.S_cond==v)[0] # all I in cond which correspond to sensitive value v\n",
    "                mean_IJ[u,v] = np.mean((K[I,:])[:,J]) \n",
    "\n",
    "\n",
    "        #final loop\n",
    "        O = copy(K[:n,:n]) #Kernel matrix for the first n elements\n",
    "        for i in range(n):\n",
    "            for j in range(i,n):\n",
    "                u = int(self.S_train[i])\n",
    "                v = int(self.S_train[j])\n",
    "    \n",
    "                O[i,j] = O[i,j] - mean_iI[i,v]  - mean_iI[j,u]  + mean_IJ[u,v] + mean_i[j] + mean_i[i] - mean_I[u] - mean_I[v]  + Ephi\n",
    "                O[j,i] = O[i,j]\n",
    "                \n",
    "        self.O = O\n",
    "        self.mean_i = mean_i\n",
    "        self.mean_iI = mean_iI\n",
    "        self.mean_I = mean_I\n",
    "        self.mean_IJ = mean_IJ\n",
    "        self.Ephi = Ephi\n",
    "                \n",
    "        return O\n",
    "    \n",
    "    \n",
    "    def build_Ot_discrete(self,Kt,St): #Kt has size testsample x 2n \n",
    "        Ot = copy(Kt[:,:self.n])\n",
    "        m = (np.shape(Kt))[0]\n",
    "    \n",
    "        mean_test_iI = np.zeros((m,self.S_max))\n",
    "        mean_test_i = np.zeros((m,1))\n",
    "        for i in range(m):\n",
    "            mean_test_i[i] = np.mean(Kt[i,self.n:])\n",
    "            for u in range(self.S_max):\n",
    "                I = self.n + np.where(self.S_cond==u)[0] # all I in cond which correspond to sensitive value u\n",
    "                mean_test_iI[i,u] = np.mean(Kt[i,I])\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(self.n): #not symmetric anymore so we need to go through all\n",
    "                u = int(St[i])\n",
    "                v = int(self.S_train[j]) \n",
    "                Ot[i,j] = Ot[i,j] - mean_test_iI[i,v]- self.mean_iI[j,u]  + self.mean_IJ[u,v]  +  self.mean_i[j] + mean_test_i[i]  - self.mean_I[u] - self.mean_I[v]  + self.Ephi # i corresponds to test, j to train; \n",
    "        self.Ot = Ot\n",
    "        return self.Ot \n",
    "    \n",
    "    \n",
    "    def predict_g(self,Kt, St): #for M-Oblivious \n",
    "        m = (np.shape(Kt))[0]#kt has size testsample x 2n \n",
    "        mean_test_iI = np.zeros((self.n,self.S_max))\n",
    "        mean_test_i = np.zeros((self.n,1))\n",
    "        for i in range(self.n):\n",
    "            mean_test_i[i] = np.mean(self.K[i,self.n:])\n",
    "            for u in range(self.S_max):\n",
    "                I = self.n + np.where(self.S_cond==u)[0] # all i in cond which correspond to sensitive value u\n",
    "                mean_test_iI[i,u] = np.mean(self.K[i,I])\n",
    "        \n",
    "        M_XS = np.matlib.repmat(mean_test_i, 1, self.S_max)-mean_test_iI\n",
    "        gOt=np.zeros((m,self.n))\n",
    "        for i in range(m):\n",
    "            gOt[i,:]=M_XS[:,St[i][0][0]]\n",
    "        \n",
    "        gOt= gOt+Kt[:,:self.n]\n",
    "\n",
    "        return gOt \n",
    "    \n",
    "    \n",
    "\n",
    "    def build_K_rbf(self,X1,X2,sigma=1):\n",
    "        # X1 has n rows = number of samples; X2 has m rows = number of samples \n",
    "        n = (np.shape(X1))[0]\n",
    "        m = (np.shape(X2))[0]\n",
    "        K_1 = np.zeros((n,m))\n",
    "    \n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                K_1[i,j] = np.exp((-1)*(np.dot(X1[i]-X2[j],X1[i]-X2[j]))/sigma)\n",
    "                \n",
    "        return K_1\n",
    "\n",
    "\n",
    "    def build_K_lin(self,X1,X2):\n",
    "        # X1 has n rows = number of samples; X2 has m rows = number of samples \n",
    "        n = (np.shape(X1))[0]\n",
    "        m = (np.shape(X2))[0]\n",
    "        K = np.zeros((n,m))\n",
    "    \n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                K[i,j] = np.dot(X1[i],X2[j])\n",
    "        \n",
    "\n",
    "        return K\n",
    "\n",
    "    \n",
    "    def Omatrix(self):\n",
    "        return self.O\n",
    "    \n",
    "    def Otmatrix(self):\n",
    "        return self.Ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import numpy.matlib\n",
    "from sklearn import svm\n",
    "from collections import namedtuple\n",
    "\n",
    "def generate_truncnorm_samples(n_samples,lower,upper,mu,sigma):\n",
    "    X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "    values = X.rvs(n_samples)\n",
    "    return values\n",
    "\n",
    "def generate_toy_data(n_samples):\n",
    "    sigma =0.5\n",
    "    unique_sensitive_feature_values=[0,1]\n",
    "    max_non_sensitive_feature_value=4.0\n",
    "    min_non_sensitive_feature_value=1.0\n",
    "    mu = 0.5*(max_non_sensitive_feature_value+min_non_sensitive_feature_value)\n",
    "    sensitive_features = [unique_sensitive_feature_values[0]] * n_samples + [unique_sensitive_feature_values[1]] * n_samples \n",
    "    sensitive_features = np.array(sensitive_features)\n",
    "    sensitive_features.shape = (len(sensitive_features), 1)\n",
    "    \n",
    "    Lower = generate_truncnorm_samples(n_samples,min_non_sensitive_feature_value,max_non_sensitive_feature_value,mu,sigma)\n",
    "    Upper = generate_truncnorm_samples(n_samples,min_non_sensitive_feature_value,max_non_sensitive_feature_value,mu,sigma)\n",
    "    non_sensitive_features0=[Lower]+[Upper]\n",
    "    non_sensitive_features0 = np.array(np.hstack(non_sensitive_features0))\n",
    "    non_sensitive_features0.shape=(len(non_sensitive_features0),1)\n",
    "    \n",
    "    \n",
    "    non_sensitive_features=[Lower-stats.bernoulli(0.9).rvs(n_samples)*1]+[Upper+stats.bernoulli(0.9).rvs(n_samples)*1]\n",
    "    non_sensitive_features = np.array(np.hstack(non_sensitive_features))\n",
    "    non_sensitive_features.shape=(len(non_sensitive_features),1)\n",
    "    \n",
    "    X = np.hstack([non_sensitive_features, sensitive_features])\n",
    "    \n",
    "    threshold=mu\n",
    "    Y_Bernoulli_params=np.array(non_sensitive_features0/max_non_sensitive_feature_value).flatten()\n",
    "    Y=np.array([stats.bernoulli(Y_Bernoulli_params[i]).rvs(1) for i in range(len(Y_Bernoulli_params))]).flatten()\n",
    "    True_Y=Y*(np.array((non_sensitive_features0>=threshold)*1).flatten())\n",
    "    Y=Y*(np.array((non_sensitive_features+sensitive_features>=threshold)*1).flatten())\n",
    "    sensitive_feature_id=np.shape(X)[1]-1\n",
    "\n",
    "    return X, Y, sensitive_feature_id, True_Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def estimate_beta_dependence(predicted_labels, sensitive_features, labels):\n",
    "    estimated_beta=0\n",
    "    n = np.size(predicted_labels)\n",
    "    unique_label_freqs =[]\n",
    "    for i in range(len(labels)):\n",
    "        unique_label_freqs.append(np.mean(predicted_labels==labels[i]))\n",
    "\n",
    "    unique_S_features=list(set(sensitive_features))\n",
    "    unique_S_freqs =[]\n",
    "    for i in range(len(unique_S_features)):\n",
    "        unique_S_freqs.append(np.mean(sensitive_features==unique_S_features[i]))\n",
    "\n",
    "    pred_feature_pairs = np.vstack((predicted_labels, sensitive_features)).T\n",
    "     \n",
    "    joint_freqs=[]       \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(unique_S_features)):\n",
    "            pattern=(labels[i],unique_S_features[j])\n",
    "            joint_freq=np.size(np.where(np.sum(np.abs(pred_feature_pairs-np.matlib.repmat(pattern, n, 1)),axis=1)==0))/n\n",
    "            joint_freqs.append(joint_freq)\n",
    "            marginal_label_freq=unique_label_freqs[i]\n",
    "            marginal_S_freq=unique_S_freqs[j]\n",
    "            estimated_beta=estimated_beta+np.abs(marginal_label_freq * marginal_S_freq-joint_freq)\n",
    "    return estimated_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = train[:445].drop('Survived', axis=1), train[:445][\"Survived\"]\n",
    "X_cond,y_cond = train[445:].drop('Survived', axis=1), train[445:][\"Survived\"]\n",
    "X_test,y_test = test.drop('Survived', axis=1), test[\"Survived\"]\n",
    "\n",
    "\n",
    "X = pd.concat((X_train, X_cond))\n",
    "y = np.concatenate((y_train,y_cond))\n",
    "S_train = X_train['Pclass']\n",
    "S_train_cond = X_cond['Pclass']\n",
    "S = np.concatenate((S_train, S_train_cond))\n",
    "S_test = X_test['Pclass']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_labels=list(set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot data\n",
    "plt.hist(X_test['Pclass'])\n",
    "sensitive_feature_values = sorted(list(set(X['Pclass'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [2**v for v in range(16,20)]    \n",
    "#Oblivious SVM\n",
    "print('OBLIVIOUS SVM')\n",
    "obl = ObliviousData()\n",
    "K = obl.build_K_lin(np.array(X),np.array(X))\n",
    "O = obl.build_O_discrete(K,S)\n",
    "Kt = obl.build_K_lin(np.array(X_test),np.array(X))\n",
    "Ot = obl.build_Ot_discrete(Kt,np.array(S_test))\n",
    "\n",
    "stdacc_list,true_acc_list,betas =[],[],[]\n",
    "    \n",
    "for c in C_list:\n",
    "    print('C: ', c)\n",
    "    clf = svm.SVC(kernel='precomputed', C=c)\n",
    "    print('Fitting OBLIVIOUS SVM')\n",
    "    clf.fit(O,y_train)\n",
    "    pred_oblv = clf.predict(Ot)\n",
    "    standard_missclassification_error=np.mean((pred_oblv==y_test)*1) # error with respect to observed labels \n",
    "    print('Standard Missclassification acc: ', standard_missclassification_error)\n",
    "    # Dependence measure\n",
    "    beta = estimate_beta_dependence(pred_oblv,X_test['Pclass'],unique_labels)\n",
    "    print('Beta-Dependence: ',beta)\n",
    "    stdacc_list.append(standard_missclassification_error)\n",
    "    betas.append(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdind=np.where(stdacc_list==np.max(stdacc_list))[0][0]\n",
    "print('(standard error, dependence):')\n",
    "print((stdacc_list[stdind], betas[stdind]))        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
